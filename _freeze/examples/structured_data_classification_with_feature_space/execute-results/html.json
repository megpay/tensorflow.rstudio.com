{
  "hash": "14ba2b8aeb21e13cfc3d430eb0730941",
  "result": {
    "markdown": "---\ntitle: Structured data classification with FeatureSpace\nauthors: \n  - \"[fchollet](https://twitter.com/fchollet)\"\n  - \"[terrytangyuan](https://github.com/terrytangyuan) - R translation\"\ndate-created: 2022/11/20\ndate-last-modified: 2022/11/20\ndescription: Classify tabular data in a few lines of code.\ncategories: [generative]\naliases: \n  - ../guide/keras/examples/structured_data_classification_with_feature_space/index.html\n---\n\n\n## Introduction\n\nThis example demonstrates how to do structured data classification\n(also known as tabular data classification), starting from a raw\nCSV file. Our data includes numerical features,\nand integer categorical features, and string categorical features.\nWe will use the utility `keras.utils.FeatureSpace` to index,\npreprocess, and encode our features.\n\nThe code is adapted from the example\n[Structured data classification from scratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/).\nWhile the previous example managed its own low-level feature preprocessing and\nencoding with Keras preprocessing layers, in this example we\ndelegate everything to `FeatureSpace`, making the workflow\nextremely quick and easy.\n\nNote that this example should be run with TensorFlow 2.12 or higher.\nBefore the release of TensorFlow 2.12, you can use `tf-nightly`.\n\n\n## The dataset\n\n[Our dataset](https://archive.ics.uci.edu/ml/datasets/heart+Disease) is provided by the\nCleveland Clinic Foundation for Heart Disease.\nIt's a CSV file with 303 rows. Each row contains information about a patient (a\n**sample**), and each column describes an attribute of the patient (a **feature**). We\nuse the features to predict whether a patient has a heart disease\n(**binary classification**).\nHere's the description of each feature:\nColumn| Description| Feature Type\n------------|--------------------|----------------------\nAge | Age in years | Numerical\nSex | (1 = male; 0 = female) | Categorical\nCP | Chest pain type (0, 1, 2, 3, 4) | Categorical\nTrestbpd | Resting blood pressure (in mm Hg on admission) | Numerical\nChol | Serum cholesterol in mg/dl | Numerical\nFBS | fasting blood sugar in 120 mg/dl (1 = true; 0 = false) | Categorical\nRestECG | Resting electrocardiogram results (0, 1, 2) | Categorical\nThalach | Maximum heart rate achieved | Numerical\nExang | Exercise induced angina (1 = yes; 0 = no) | Categorical\nOldpeak | ST depression induced by exercise relative to rest | Numerical\nSlope | Slope of the peak exercise ST segment | Numerical\nCA | Number of major vessels (0-3) colored by fluoroscopy | Both numerical & categorical\nThal | 3 = normal; 6 = fixed defect; 7 = reversible defect | Categorical\nTarget | Diagnosis of heart disease (1 = true; 0 = false) | Target\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tfdatasets)\n```\n:::\n\n\n## Preparing the data\n\nLet's download the data and load it into a dataframe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read.csv2(\"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\", header = TRUE, sep = \",\")\n```\n:::\n\n\nThe dataset includes 303 samples with 14 columns per sample\n(13 features, plus the target label):\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(df)\n```\n:::\n\n\nHere's a preview of a few samples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(df)\n```\n:::\n\n\nThe last column, \"target\", indicates whether the patient\nhas a heart disease (1) or not (0).\nLet's split the data into a training and validation set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nval_df <- df[sample(nrow(df), ceiling(nrow(df)*0.2)), ]\ntrain_df <- df[-as.numeric(rownames(val_df)), ]\nsprintf(\"Using %d samples for training and %d for validation\", nrow(train_df), nrow(val_df))\n```\n:::\n\n\nLet's generate `tf.data.Dataset` objects for each dataframe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_to_dataset <- function(df) {\n  features <- df[,-ncol(df)]\n  labels <- df[ncol(df)]\n  ds <- tensor_slices_dataset(list(features, labels))\n  df <- ds$shuffle(buffer_size = nrow(df))\n  return(df)\n}\ntrain_ds <- df_to_dataset(train_df)\nval_ds <- df_to_dataset(val_df)\n```\n:::\n\n\nEach `Dataset` yields a tuple `(input, target)` where `input` is a dictionary of features\nand `target` is the value `0` or `1`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout = train_ds$take(1L) %>% as_array_iterator() %>% iterate(simplify = FALSE)\nsprintf(\"Dataset: %s\", out[[1]])\n```\n:::\n\n\nLet's batch the datasets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_ds <- train_ds$batch(32L)\nval_ds <- val_ds$batch(32L)\n```\n:::\n\n\n## Configuring a `FeatureSpace`\n\nTo configure how each feature should be preprocessed,\nwe instantiate a `keras.utils.FeatureSpace`, and we\npass to it a dictionary that maps the name of our features\nto a string that describes the feature type.\n\nWe have a few \"integer categorical\" features such as `\"FBS\"`,\none \"string categorical\" feature (`\"thal\"`),\nand a few numerical features, which we'd like to normalize\n-- except `\"age\"`, which we'd like to discretize into\na number of bins.\n\nWe also use the `crosses` argument\nto capture *feature interactions* for some categorical\nfeatures, that is to say, create additional features\nthat represent value co-occurrences for these categorical features.\nYou can compute feature crosses like this for arbitrary sets of\ncategorical features -- not just tuples of two features.\nBecause the resulting co-occurences are hashed\ninto a fixed-sized vector, you don't need to worry about whether\nthe co-occurence space is too large.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# TBA: FeatureSpace module is not available yet:\n# * https://github.com/keras-team/keras-io/blob/master/examples/structured_data/structured_data_classification_with_feature_space.py#L159\n# * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/utils/all_utils.py\n# * https://github.com/keras-team/keras/blob/master/keras/utils/__init__.py#L53\nprint(\"TBA\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}