{
  "hash": "d44e2f4708c1cc95114b7a3ee46b53b3",
  "result": {
    "markdown": "---\ntitle: \"Timeseries classification from scratch\"\nauthors:\n  - \"[hfawaz](https://github.com/hfawaz/)\"\n  - \"[terrytangyuan](https://github.com/terrytangyuan) - R adaptation\"\n  - \"[t-kalinowski](https://github.com/t-kalinowski) - R adaptation\"\ndate-created: 2022/12/03\ndate-last-modified: 2022/12/03\ndescription: \"Training a timeseries classifier from scratch on the FordA dataset from the UCR/UEA archive.\"\ncategories: [timeseries]\naliases:\n  - ../guide/keras/examples/timeseries_classification_from_scratch/index.html\n---\n\n\n## Introduction\n\nThis example shows how to do timeseries classification from scratch, starting from raw\nCSV timeseries files on disk. We demonstrate the workflow on the FordA dataset from the\n[UCR/UEA archive](https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/).\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset.seed(1234)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA\"\n\ntrain_df <- \"FordA_TRAIN.tsv\" %>%\n  get_file(., file.path(url, .)) %>%\n  readr::read_tsv(col_names = FALSE)\nx_train <- train_df[, -1]\ny_train <- train_df[, 1]\n\ntest_df <- \"FordA_TEST.tsv\" %>%\n  get_file(., file.path(url, .)) %>%\n  readr::read_tsv(col_names = FALSE)\nx_test <- test_df[, -1]\ny_test <- test_df[, 1]\n```\n:::\n\n\n## Visualize the data\n\nHere we visualize one timeseries example for each class in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggfortify)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n```{.r .cell-code}\nts_obj <- ts(tibble::tibble(\"Class -1\" = as.numeric(x_train[1, ]), \"Class 1\" = as.numeric(x_train[2, ])))\nautoplot(ts_obj, ts.geom = 'line', facets = FALSE)\n```\n\n::: {.cell-output-display}\n![](timeseries_classification_from_scratch_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Standardize the data\n\nOur timeseries are already in a single length (500). However, their values are\nusually in various ranges. This is not ideal for a neural network;\nin general we should seek to make the input values normalized.\nFor this specific dataset, the data is already z-normalized: each timeseries sample\nhas a mean equal to zero and a standard deviation equal to one. This type of\nnormalization is very common for timeseries classification problems, see\n[Bagnall et al. (2016)](https://link.springer.com/article/10.1007/s10618-016-0483-9).\n\nIn order to use `sparse_categorical_crossentropy`, we will have to count\nthe number of classes beforehand.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_classes <- length(unique(y_train[[1]]))\n```\n:::\n\n\nNow we shuffle the training set because we will be using the `validation_split` option\nlater when training.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_ind <- sample(nrow(x_train))\nx_train <- x_train[train_ind,]\ny_train <- y_train[train_ind,]\n```\n:::\n\n\nStandardize the labels to positive integers.\nThe expected labels will then be 0 and 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_train[y_train == -1, ] <- 0\ny_test[y_test == -1, ] <- 0\n```\n:::\n\n\nNote that the timeseries data used here are univariate, meaning we only have one channel\nper timeseries example.\nWe will therefore transform the timeseries into a multivariate one with one channel\nusing a simple reshaping via numpy.\nThis will allow us to construct a model that is easily applicable to multivariate time\nseries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_train <- as.matrix(x_train)\ndim(x_train) <- c(dim(x_train), 1)\nx_test <- as.matrix(x_test)\ndim(x_test) <- c(dim(x_test), 1)\n```\n:::\n\n\n## Build a model\n\nWe build a Fully Convolutional Neural Network originally proposed in\n[this paper](https://arxiv.org/abs/1611.06455).\nThe implementation is based on the TF 2 version provided\n[here](https://github.com/hfawaz/dl-4-tsc/).\nThe following hyperparameters (kernel_size, filters, the usage of BatchNorm) were found\nvia random search using [KerasTuner](https://github.com/keras-team/keras-tuner).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninput_shape <- dim(x_train)[-1]\ninput_layer <- layer_input(input_shape)\n\noutput_layer <- input_layer %>%\n  # First convolutional layer\n  layer_conv_1d(filters = 64, kernel_size = 3, padding = \"same\") %>% \n  layer_batch_normalization() %>%\n  layer_activation_relu() %>%\n  # Second convolutional layer\n  layer_conv_1d(filters = 64, kernel_size = 3, padding = \"same\") %>% \n  layer_batch_normalization() %>%\n  layer_activation_relu() %>%\n  # Third convolutional layer\n  layer_conv_1d(filters = 64, kernel_size = 3, padding = \"same\") %>% \n  layer_batch_normalization() %>%\n  layer_activation_relu() %>%\n  layer_global_average_pooling_1d() %>%\n  layer_dense(num_classes, activation = \"softmax\")\n\nmodel <- keras_model(inputs = input_layer, outputs = output_layer)\n```\n:::\n\n\n\n## Train the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepochs <- 500\nbatch_size <- 32\ncallbacks <- list(\n  callback_model_checkpoint(\"best_model.h5\", save_best_only = TRUE, monitor = \"val_loss\"),\n  callback_reduce_lr_on_plateau(monitor = \"val_loss\", factor = 0.5, patience = 20, min_lr = 0.0001),\n  callback_early_stopping(monitor = \"val_loss\", patience = 50, verbose = 1)\n)\n\nmodel %>% compile(\n  optimizer = \"adam\",\n  loss = \"sparse_categorical_crossentropy\",\n  metrics = list(\"sparse_categorical_accuracy\")\n)\nhistory <- model %>%\n  fit(x_train, as.matrix(y_train),\n    batch_size = batch_size,\n    epochs = epochs,\n    callbacks = callbacks,\n    validation_split = 0.2,\n    verbose = 1)\n```\n:::\n\n\n## Evaluate model on test data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloaded_model <- load_model_hdf5(\"best_model.h5\")\nresult <- loaded_model %>% evaluate(x_test, as.matrix(y_test))\n\nsprintf(\"Test loss: %s\", result[[\"loss\"]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Test loss: 0.0945346355438232\"\n```\n:::\n\n```{.r .cell-code}\nsprintf(\"Test accuracy: %s\", result[[\"sparse_categorical_accuracy\"]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Test accuracy: 0.971212148666382\"\n```\n:::\n:::\n\n\n## Plot the model's training and validation loss\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n\n::: {.cell-output-display}\n![](timeseries_classification_from_scratch_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nWe can see how the training accuracy reaches almost 0.95 after 100 epochs. However, by observing the validation accuracy we can see how the network still needs training until it reaches almost 0.97 for both the validation and the training accuracy after 200 epochs. Beyond the 200th epoch, if we continue on training, the validation accuracy will start decreasing while the training accuracy will continue on increasing: the model starts overfitting.\n",
    "supporting": [
      "timeseries_classification_from_scratch_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}