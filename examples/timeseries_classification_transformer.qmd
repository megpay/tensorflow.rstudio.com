---
title: "Timeseries classification with a Transformer model"
authors:
  - "[Theodoros Ntakouris](https://github.com/ntakouris)"
  - "[terrytangyuan](https://github.com/terrytangyuan) - R adaptation"
  - "[t-kalinowski](https://github.com/t-kalinowski) - R adaptation"
date-created: 2022/12/12
date-last-modified: 2022/12/12
description: "This notebook demonstrates how to do timeseries classification using a Transformer model."
categories: [timeseries]
aliases:
  - ../guide/keras/examples/timeseries_classification_transformer/index.html
---

## Introduction

This is the Transformer architecture from
[Attention Is All You Need](https://arxiv.org/abs/1706.03762),
applied to timeseries instead of natural language.

This example requires TensorFlow 2.4 or higher.


## Load the dataset

We are going to use the same dataset and preprocessing as the
[TimeSeries Classification from Scratch](https://tensorflow.rstudio.com/examples/timeseries_classification_from_scratch)
example.

```{r}
library(tensorflow)
library(keras)
set.seed(1234)
```

```{r, message=FALSE}
url <- "https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA"
train_df <- "FordA_TRAIN.tsv" %>%
  get_file(., file.path(url, .)) %>%
  readr::read_tsv(col_names = FALSE)
x_train <- train_df[, -1]
y_train <- train_df[, 1]
test_df <- "FordA_TEST.tsv" %>%
  get_file(., file.path(url, .)) %>%
  readr::read_tsv(col_names = FALSE)
x_test <- test_df[, -1]
y_test <- test_df[, 1]

n_classes <- length(unique(y_train[[1]]))

train_ind <- sample(nrow(x_train))
x_train <- x_train[train_ind,]
y_train <- y_train[train_ind,]

y_train[y_train == -1, ] <- 0
y_test[y_test == -1, ] <- 0

x_train <- as.matrix(x_train)
dim(x_train) <- c(dim(x_train), 1)
x_test <- as.matrix(x_test)
dim(x_test) <- c(dim(x_test), 1)
```

## Build a model

Our model processes a tensor of shape `(batch size, sequence length, features)`,
where `sequence length` is the number of time steps and `features` is each input
timeseries.

You can replace your classification RNN layers with this one: the
inputs are fully compatible!

We include residual connections, layer normalization, and dropout.
The resulting layer can be stacked multiple times.
The projection layers are implemented through `layer_conv_1d()`.

```{r}
transformer_encoder <- function(inputs, head_size, num_heads, ff_dim, dropout = 0) {
  # Attention and Normalization
  x <- layer_multi_head_attention(key_dim = head_size, num_heads = num_heads, dropout = dropout)(inputs, inputs) %>% 
    layer_dropout(dropout) %>% 
    layer_layer_normalization(epsilon=1e-6)
  res <- x + inputs
  
  # Feed Forward Part
  x <- res %>%
    layer_conv_1d(filters = ff_dim, kernel_size = 1, activation = "relu") %>% 
    layer_dropout(dropout) %>% 
    layer_conv_1d(filters = dim(inputs)[length(dim(inputs))], kernel_size = 1) %>% 
    layer_layer_normalization(epsilon=1e-6)()
  return(x + res)
}

build_model <- function(
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout = 0,
    mlp_dropout = 0
) {
  inputs <- layer_input(input_shape)
  x <- inputs
  for (i in 1:num_transformer_blocks) {
    x <- x %>% transformer_encoder(head_size, num_heads, ff_dim, dropout)
  }
  x <- x %>% layer_global_average_pooling_1d(data_format = "channels_first")
  for (dim in mlp_units) {
    x <- x %>%
      layer_dense(dim, activation = "relu") %>% 
      layer_dropout(mlp_dropout)
  }
  outputs <- x %>% layer_dense(n_classes, activation = "softmax")
  return(keras_model(inputs = inputs, outputs = outputs))
}
```


## Train and evaluate

```{r}
input_shape <- dim(x_train)[-1]
model <- build_model(
  input_shape,
  head_size = 256,
  num_heads = 4,
  ff_dim = 4,
  num_transformer_blocks = 4,
  mlp_units = c(128),
  mlp_dropout = 0.4,
  dropout = 0.25
)
model %>% compile(
  loss = "sparse_categorical_crossentropy",
  optimizer = optimizer_adam(learning_rate = 1e-4),
  metrics = c("sparse_categorical_accuracy"),
)
summary(model)

callbacks <- list(
  callback_early_stopping(patience = 10, restore_best_weights = TRUE)
)

history <- model %>%
  fit(x_train, as.matrix(y_train),
    batch_size = 64,
    epochs = 200,
    callbacks = callbacks,
    validation_split = 0.2)

model %>% evaluate(x_test, as.matrix(y_test), verbose = 1)
```

## Conclusions

In about 110-120 epochs (25s each on Colab), the model reaches a training
accuracy of ~0.95, validation accuracy of ~84 and a testing
accuracy of ~85, without hyperparameter tuning. And that is for a model
with less than 100k parameters. Of course, parameter count and accuracy could be
improved by a hyperparameter search and a more sophisticated learning rate
schedule, or a different optimizer.

You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/timeseries_transformer_classification)
and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/timeseries_transformer_classification).
