{
  "hash": "b438c9771a5b4314b9766a246afae185",
  "result": {
    "markdown": "---\ntitle: Introduction to Tensors\naliases:\n    - ../../guide/tensorflow/tensors/index.html\ndescription: >\n  Learn about Tensors, the multi-dimensional arrays used by TensorFlow.\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n```\n:::\n\n\nTensors are multi-dimensional arrays with a uniform type (called a\n`dtype`). You can see all supported `dtypes` with `names(tf$dtypes)`.\n\nIf you're familiar with R array or\n[NumPy](https://numpy.org/devdocs/user/quickstart.html), tensors are\n(kind of) like R or NumPy arrays.\n\nAll tensors are immutable: you can never update the contents of a\ntensor, only create a new one.\n\n## Basics\n\nLet's create some basic tensors.\n\nHere is a \"scalar\" or \"rank-0\" tensor . A scalar contains a single\nvalue, and no \"axes\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# This will be an float64 tensor by default; see \"dtypes\" below.\nrank_0_tensor <- as_tensor(4)\nprint(rank_0_tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(4.0, shape=(), dtype=float64)\n```\n:::\n:::\n\n\nA \"vector\" or \"rank-1\" tensor is like a list of values. A vector has one\naxis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrank_1_tensor <- as_tensor(c(2, 3, 4))\nprint(rank_1_tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([2. 3. 4.], shape=(3), dtype=float64)\n```\n:::\n:::\n\n\nA \"matrix\" or \"rank-2\" tensor has two axes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# If you want to be specific, you can set the dtype (see below) at creation time\nrank_2_tensor <- \n  as_tensor(rbind(c(1, 2), \n                  c(3, 4), \n                  c(5, 6)), \n            dtype=tf$float16)\nprint(rank_2_tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 2.]\n [3. 4.]\n [5. 6.]], shape=(3, 2), dtype=float16)\n```\n:::\n:::\n\n\n| A scalar, shape: `[]`                               | A vector, shape: `[3]`                                                               | A matrix, shape: `[3, 2]`                                                    |\n|-----------------------------------------------------|--------------------------------------------------------------------------------------|------------------------------------------------------------------------------|\n| ![A scalar, the number 4](images/tensor/scalar.png) | ![The line with 3 sections, each one containing a number.](images/tensor/vector.png) | ![A 3x2 grid, with each cell containing a number.](images/tensor/matrix.png) |\n\nTensors may have more axes; here is a tensor with three axes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# There can be an arbitrary number of\n# axes (sometimes called \"dimensions\")\n\nrank_3_tensor <- as_tensor(0:29, shape = c(3, 2, 5))\nrank_3_tensor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]]\n\n [[10 11 12 13 14]\n  [15 16 17 18 19]]\n\n [[20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n```\n:::\n:::\n\n\nThere are many ways you might visualize a tensor with more than two\naxes.\n\n| A 3-axis tensor, shape: `[3, 2, 5]`                                                                          |\n|--------------------------------------------------------------------------------------------------------------|\n|                                                                                                              |\n| ![](images/tensor/3-axis_numpy.png)! ![](images/tensor/3-axis_numpy.png) ![](images/tensor/3-axis_block.png) |\n\nYou can convert a tensor to an R array using `as.array()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.array(rank_2_tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n```\n:::\n:::\n\n\nTensors often contain floats and ints, but have many other types,\nincluding:\n\n-   complex numbers\n-   strings\n\nThe base `tf$Tensor` class requires tensors to be \"rectangular\"---that\nis, along each axis, every element is the same size. However, there are\nspecialized types of tensors that can handle different shapes:\n\n-   Ragged tensors (see [RaggedTensor](#ragged_tensors) below)\n-   Sparse tensors (see [SparseTensor](#sparse_tensors) below)\n\nYou can do basic math on tensors, including addition, element-wise\nmultiplication, and matrix multiplication.\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- as_tensor(1:4, shape = c(2, 2)) \nb <- as_tensor(1L, shape = c(2, 2))\n\na + b # element-wise addition, same as tf$add(a, b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[2 3]\n [4 5]], shape=(2, 2), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\na * b # element-wise multiplication, same as tf$multiply(a, b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1 2]\n [3 4]], shape=(2, 2), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\ntf$matmul(a, b) # matrix multiplication\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[3 3]\n [7 7]], shape=(2, 2), dtype=int32)\n```\n:::\n:::\n\n\nTensors are used in all kinds of operations (ops).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(rbind(c(4, 5), c(10, 1)))\n\n# Find the largest value\n\n# Find the largest value\ntf$reduce_max(x) # can also just call max(c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(10.0, shape=(), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n# Find the index of the largest value\ntf$math$argmax(x) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1 0], shape=(2), dtype=int64)\n```\n:::\n\n```{.r .cell-code}\ntf$nn$softmax(x) # Compute the softmax\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[2.68941421e-01 7.31058579e-01]\n [9.99876605e-01 1.23394576e-04]], shape=(2, 2), dtype=float64)\n```\n:::\n:::\n\n\n## About shapes\n\nTensors have shapes. Some vocabulary:\n\n-   **Shape**: The length (number of elements) of each of the axes of a\n    tensor.\n-   **Rank**: Number of tensor axes. A scalar has rank 0, a vector has\n    rank 1, a matrix is rank 2.\n-   **Axis** or **Dimension**: A particular dimension of a tensor.\n-   **Size**: The total number of items in the tensor, the product of\n    the shape vector's elements.\n\nNote: Although you may see reference to a \"tensor of two dimensions\", a\nrank-2 tensor does not usually describe a 2D space.\n\nTensors and `tf$TensorShape` objects have convenient properties for\naccessing these:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrank_4_tensor <- tf$zeros(shape(3, 2, 4, 5))\n```\n:::\n\n\n| A rank-4 tensor, shape: `[3, 2, 4, 5]`                       |\n|--------------------------------------------------------------|\n| ![A tensor shape is like a vector.](images/tensor/shape.png) |\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmessage(\"Type of every element: \", rank_4_tensor$dtype)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nType of every element: <dtype: 'float32'>\n```\n:::\n\n```{.r .cell-code}\nmessage(\"Number of axes: \", length(dim(rank_4_tensor)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNumber of axes: 4\n```\n:::\n\n```{.r .cell-code}\nmessage(\"Shape of tensor: \", dim(rank_4_tensor)) # can also access via rank_4_tensor$shape\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nShape of tensor: 3245\n```\n:::\n\n```{.r .cell-code}\nmessage(\"Elements along axis 0 of tensor: \", dim(rank_4_tensor)[1])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nElements along axis 0 of tensor: 3\n```\n:::\n\n```{.r .cell-code}\nmessage(\"Elements along the last axis of tensor: \", dim(rank_4_tensor) |> tail(1)) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nElements along the last axis of tensor: 5\n```\n:::\n\n```{.r .cell-code}\nmessage(\"Total number of elements (3*2*4*5): \", length(rank_4_tensor)) # can also call tf$size()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTotal number of elements (3*2*4*5): 120\n```\n:::\n:::\n\n\nWhile axes are often referred to by their indices, you should always\nkeep track of the meaning of each. Often axes are ordered from global to\nlocal: The batch axis first, followed by spatial dimensions, and\nfeatures for each location last. This way feature vectors are contiguous\nregions of memory.\n\n| Typical axis order                                                                                                     |\n|------------------------------------------------------------------------------------------------------------------------|\n| ![Keep track of what each axis is. A 4-axis tensor might be: Batch, Width, Height, Features](images/tensor/shape2.png) |\n\n## Indexing\n\n### Single-axis indexing\n\nSee `` ?`[.tensorflow.tensor` `` for details\n\n### Multi-axis indexing\n\nHigher rank tensors are indexed by passing multiple indices.\n\nThe exact same rules as in the single-axis case apply to each axis\nindependently.\n\nRead the [tensor slicing\nguide](https://tensorflow.org/guide/tensor_slicing) to learn how you can\napply indexing to manipulate individual elements in your tensors.\n\n## Manipulating Shapes\n\nReshaping a tensor is of great utility.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shape returns a `TensorShape` object that shows the size along each axis\n\nx <- as_tensor(1:3, shape = c(1, -1)) \nx$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([1, 3])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# You can convert this object into an R vector too\nas.integer(x$shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 3\n```\n:::\n:::\n\n\nYou can reshape a tensor into a new shape. The `tf$reshape` operation is\nfast and cheap as the underlying data does not need to be duplicated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# You can reshape a tensor to a new shape.\n# Note that you're passing in integers\n\nreshaped <- tf$reshape(x, c(1L, 3L))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([1, 3])\n```\n:::\n\n```{.r .cell-code}\nreshaped$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([1, 3])\n```\n:::\n:::\n\n\nThe data maintains its layout in memory and a new tensor is created,\nwith the requested shape, pointing to the same data. TensorFlow uses\nC-style \"row-major\" memory ordering, where incrementing the rightmost\nindex corresponds to a single step in memory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrank_3_tensor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]]\n\n [[10 11 12 13 14]\n  [15 16 17 18 19]]\n\n [[20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n```\n:::\n:::\n\n\nIf you flatten a tensor you can see what order it is laid out in memory.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A `-1` passed in the `shape` argument says \"Whatever fits\".\ntf$reshape(rank_3_tensor, c(-1L))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29], shape=(30), dtype=int32)\n```\n:::\n:::\n\n\nA typical and reasonable use of `tf$reshape` is to combine or split\nadjacent axes (or add/remove `1`s).\n\nFor this 3x2x5 tensor, reshaping to (3x2)x5 or 3x(2x5) are both\nreasonable things to do, as the slices do not mix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$reshape(rank_3_tensor, as.integer(c(3*2, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 0  1  2  3  4]\n [ 5  6  7  8  9]\n [10 11 12 13 14]\n [15 16 17 18 19]\n [20 21 22 23 24]\n [25 26 27 28 29]], shape=(6, 5), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\ntf$reshape(rank_3_tensor, as.integer(c(3L, -1L)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 0  1  2  3  4  5  6  7  8  9]\n [10 11 12 13 14 15 16 17 18 19]\n [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n```\n:::\n:::\n\n\n| Some good reshapes.                                                                                                                  |\n|--------------------------------------------------------------------------------------------------------------------------------------|\n| ![A 3x2x5 tensor](images/tensor/reshape-before.png) ![3x10](images/tensor/reshape-good1.png) ![6x5](images/tensor/reshape-good2.png) |\n\nhttps://www.tensorflow.org/guide/images/tensor/reshape-before.png\nhttps://www.tensorflow.org/guide/\nhttps://www.tensorflow.org/guide/images/tensor/reshape-good2.png\n\nReshaping will \"work\" for any new shape with the same total number of\nelements, but it will not do anything useful if you do not respect the\norder of the axes.\n\nSwapping axes in `tf$reshape` does not work; you need `tf$transpose` for\nthat.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bad examples: don't do this\n\n# You can't reorder axes with reshape.\ntf$reshape(rank_3_tensor, as.integer(c(2, 3, 5)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[[ 0  1  2  3  4]\n  [ 5  6  7  8  9]\n  [10 11 12 13 14]]\n\n [[15 16 17 18 19]\n  [20 21 22 23 24]\n  [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\n# This is a mess\ntf$reshape(rank_3_tensor, as.integer(c(5, 6)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]\n [24 25 26 27 28 29]], shape=(5, 6), dtype=int32)\n```\n:::\n\n```{.r .cell-code}\n# This doesn't work at all\ntry(tf$reshape(rank_3_tensor, as.integer(c(7, -1))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:GPU:0}} Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]\n```\n:::\n:::\n\n\n| Some bad reshapes.                                                                                                                                                                                                                                            |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ![You can\\'t reorder axes, use tf\\$transpose for that](images/tensor/reshape-bad.png) ![Anything that mixes the slices of data together is probably wrong.](images/tensor/reshape-bad4.png) ![The new shape must fit exactly](images/tensor/reshape-bad2.png) |\n\nYou may run across not-fully-specified shapes. Either the shape contains\na `NULL` (an axis-length is unknown) or the whole shape is `NULL` (the\nrank of the tensor is unknown).\n\nExcept for [`tf$RaggedTensor`](#ragged_tensors), such shapes will only\noccur in the context of TensorFlow's symbolic, graph-building APIs:\n\n-   [tf_function](function.qmd)\n-   The [keras functional\n    API](https://www.tensorflow.org/guide/keras/functional).\n\n## More on `DTypes`\n\nTo inspect a `tf$Tensor`'s data type use the `Tensor$dtype` property.\n\nWhen creating a `tf$Tensor` from a Python object you may optionally\nspecify the datatype.\n\nIf you don't, TensorFlow chooses a datatype that can represent your\ndata. TensorFlow converts R integers to `tf$int32` and R floating point\nnumbers to `tf$float64`.\n\nYou can cast from type to type.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nthe_f64_tensor <- as_tensor(c(2.2, 3.3, 4.4), dtype = tf$float64)\nthe_f16_tensor <- tf$cast(the_f64_tensor, dtype = tf$float16)\n# Now, cast to an uint8 and lose the decimal precision\n\nthe_u8_tensor <- tf$cast(the_f16_tensor, dtype = tf$uint8)\nthe_u8_tensor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([2 3 4], shape=(3), dtype=uint8)\n```\n:::\n:::\n\n\n## Broadcasting\n\nBroadcasting is a concept borrowed from the [equivalent feature in\nNumPy](https://numpy.org/doc/stable/user/basics.html). In short, under\ncertain conditions, smaller tensors are recycled automatically to fit\nlarger tensors when running combined operations on them.\n\nThe simplest and most common case is when you attempt to multiply or add\na tensor to a scalar. In that case, the scalar is broadcast to be the\nsame shape as the other argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(c(1, 2, 3))\n\ny <- as_tensor(2)\nz <- as_tensor(c(2, 2, 2))\n\n# All of these are the same computation\ntf$multiply(x, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\nx * y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\nx * z\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([2. 4. 6.], shape=(3), dtype=float64)\n```\n:::\n:::\n\n\nLikewise, axes with length 1 can be stretched out to match the other\narguments. Both arguments can be stretched in the same computation.\n\nIn this case a 3x1 matrix is element-wise multiplied by a 1x4 matrix to\nproduce a 3x4 matrix. Note how the leading 1 is optional: The shape of y\nis `[4]`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# These are the same computations\n(x <- tf$reshape(x, as.integer(c(3, 1))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1.]\n [2.]\n [3.]], shape=(3, 1), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\n(y <- tf$range(1, 5,  dtype = \"float64\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([1. 2. 3. 4.], shape=(4), dtype=float64)\n```\n:::\n\n```{.r .cell-code}\nx * y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 1.  2.  3.  4.]\n [ 2.  4.  6.  8.]\n [ 3.  6.  9. 12.]], shape=(3, 4), dtype=float64)\n```\n:::\n:::\n\n\n| A broadcasted add: a `[3, 1]` times a `[1, 4]` gives a `[3,4]`                                   |\n|--------------------------------------------------------------------------------------------------|\n| ![Adding a 3x1 matrix to a 4x1 matrix results in a 3x4 matrix](images/tensor/broadcasting.png)\\\\ |\n\nHere is the same operation without broadcasting:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_stretch <- as_tensor(rbind(c(1, 1, 1, 1),\n                             c(2, 2, 2, 2),\n                             c(3, 3, 3, 3)))\n\ny_stretch <- as_tensor(rbind(c(1, 2, 3, 4),\n                             c(1, 2, 3, 4),\n                             c(1, 2, 3, 4)))\n\nx_stretch * y_stretch  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 1.  2.  3.  4.]\n [ 2.  4.  6.  8.]\n [ 3.  6.  9. 12.]], shape=(3, 4), dtype=float64)\n```\n:::\n:::\n\n\nMost of the time, broadcasting is both time and space efficient, as the\nbroadcast operation never materializes the expanded tensors in memory.\n\nYou see what broadcasting looks like using `tf$broadcast_to`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$broadcast_to(as_tensor(c(1, 2, 3)), c(3L, 3L))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]], shape=(3, 3), dtype=float64)\n```\n:::\n:::\n\n\nUnlike a mathematical op, for example, `broadcast_to` does nothing\nspecial to save memory. Here, you are materializing the tensor.\n\nIt can get even more complicated. [This\nsection](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html)\nof Jake VanderPlas's book *Python Data Science Handbook* shows more\nbroadcasting tricks (again in NumPy).\n\n## `tf$convert_to_tensor`\n\nMost ops, like `tf$matmul` and `tf$reshape` take arguments of class\n`tf$Tensor`. However, you'll notice in the above case, objects shaped\nlike tensors are also accepted.\n\nMost, but not all, ops call `convert_to_tensor` on non-tensor arguments.\nThere is a registry of conversions, and most object classes like NumPy's\n`ndarray`, `TensorShape`, Python lists, and `tf$Variable` will all\nconvert automatically.\n\nSee `tf$register_tensor_conversion_function` for more details, and if\nyou have your own type you'd like to automatically convert to a tensor.\n\n## Ragged Tensors\n\nA tensor with variable numbers of elements along some axis is called\n\"ragged\". Use `tf$ragged$RaggedTensor` for ragged data.\n\nFor example, This cannot be represented as a regular tensor:\n\n| A `tf$RaggedTensor`, shape: `[4, NULL]`                                                    |\n|--------------------------------------------------------------------------------------------|\n| ![A 2-axis ragged tensor, each row can have a different length.](images/tensor/ragged.png) |\n\n\n::: {.cell}\n\n```{.r .cell-code}\nragged_list <- list(list(0, 1, 2, 3),\n                    list(4, 5),\n                    list(6, 7, 8),\n                    list(9))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntry(tensor <- as_tensor(ragged_list))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  ValueError: Can't convert non-rectangular Python sequence to Tensor.\n```\n:::\n:::\n\n\nInstead create a `tf$RaggedTensor` using `tf$ragged$constant`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(ragged_tensor <- tf$ragged$constant(ragged_list))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.RaggedTensor [[0.0, 1.0, 2.0, 3.0], [4.0, 5.0], [6.0, 7.0, 8.0], [9.0]]>\n```\n:::\n:::\n\n\nThe shape of a `tf$RaggedTensor` will contain some axes with unknown\nlengths:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(ragged_tensor$shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([4, None])\n```\n:::\n:::\n\n\n## String tensors\n\n`tf$string` is a `dtype`, which is to say you can represent data as\nstrings (variable-length byte arrays) in tensors.\n\nThe length of the string is not one of the axes of the tensor. See\n`tf$strings` for functions to manipulate them.\n\nHere is a scalar string tensor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tensors can be strings, too here is a scalar string.\n\n(scalar_string_tensor <- as_tensor(\"Gray wolf\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(b'Gray wolf', shape=(), dtype=string)\n```\n:::\n:::\n\n\nAnd a vector of strings:\n\n| A vector of strings, shape: `[3,]`                                                |\n|-----------------------------------------------------------------------------------|\n| ![The string length is not one of the tensor\\'s axes.](images/tensor/strings.png) |\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntensor_of_strings <- as_tensor(c(\"Gray wolf\",\n                                 \"Quick brown fox\",\n                                 \"Lazy dog\"))\n# Note that the shape is (3). The string length is not included.\n\ntensor_of_strings\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3), dtype=string)\n```\n:::\n:::\n\n\nIn the above printout the `b` prefix indicates that `tf$string` dtype is\nnot a unicode string, but a byte-string. See the [Unicode\nTutorial](https://www.tensorflow.org/tutorials/load_data/unicode) for\nmore about working with unicode text in TensorFlow.\n\nIf you pass unicode characters they are utf-8 encoded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_tensor(\"🥳👍\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(b'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d', shape=(), dtype=string)\n```\n:::\n:::\n\n\nSome basic functions with strings can be found in `tf$strings`,\nincluding `tf$strings$split`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# You can use split to split a string into a set of tensors\ntf$strings$split(scalar_string_tensor, sep=\" \")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([b'Gray' b'wolf'], shape=(2), dtype=string)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# ...and it turns into a `RaggedTensor` if you split up a tensor of strings,\n# as each string might be split into a different number of parts.\ntf$strings$split(tensor_of_strings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.RaggedTensor [[b'Gray', b'wolf'], [b'Quick', b'brown', b'fox'], [b'Lazy', b'dog']]>\n```\n:::\n:::\n\n\n| Three strings split, shape: `[3, NULL]`                                                  |\n|------------------------------------------------------------------------------------------|\n| ![Splitting multiple strings returns a tf\\$RaggedTensor](images/tensor/string-split.png) |\n\nAnd `tf$string$to_number`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext <- as_tensor(\"1 10 100\")\ntf$strings$to_number(tf$strings$split(text, \" \"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([  1.  10. 100.], shape=(3), dtype=float32)\n```\n:::\n:::\n\n\nAlthough you can't use `tf$cast` to turn a string tensor into numbers,\nyou can convert it into bytes, and then into numbers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbyte_strings <- tf$strings$bytes_split(as_tensor(\"Duck\"))\nbyte_ints <- tf$io$decode_raw(as_tensor(\"Duck\"), tf$uint8)\ncat(\"Byte strings: \"); print(byte_strings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nByte strings: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([b'D' b'u' b'c' b'k'], shape=(4), dtype=string)\n```\n:::\n\n```{.r .cell-code}\ncat(\"Bytes: \"); print(byte_ints)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBytes: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([ 68 117  99 107], shape=(4), dtype=uint8)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Or split it up as unicode and then decode it\nunicode_bytes <- as_tensor(\"アヒル 🦆\")\nunicode_char_bytes <- tf$strings$unicode_split(unicode_bytes, \"UTF-8\")\nunicode_values <- tf$strings$unicode_decode(unicode_bytes, \"UTF-8\")\n\ncat(\"Unicode bytes: \"); unicode_bytes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnicode bytes: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(b'\\xe3\\x82\\xa2\\xe3\\x83\\x92\\xe3\\x83\\xab \\xf0\\x9f\\xa6\\x86', shape=(), dtype=string)\n```\n:::\n\n```{.r .cell-code}\ncat(\"Unicode chars: \"); unicode_char_bytes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnicode chars: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([b'\\xe3\\x82\\xa2' b'\\xe3\\x83\\x92' b'\\xe3\\x83\\xab' b' ' b'\\xf0\\x9f\\xa6\\x86'], shape=(5), dtype=string)\n```\n:::\n\n```{.r .cell-code}\ncat(\"Unicode values: \"); unicode_values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnicode values: \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([ 12450  12498  12523     32 129414], shape=(5), dtype=int32)\n```\n:::\n:::\n\n\nThe `tf$string` dtype is used for all raw bytes data in TensorFlow. The\n`tf$io` module contains functions for converting data to and from bytes,\nincluding decoding images and parsing csv.\n\n## Sparse tensors\n\nSometimes, your data is sparse, like a very wide embedding space.\nTensorFlow supports `tf$sparse$SparseTensor` and related operations to\nstore sparse data efficiently.\n\n| A `tf$SparseTensor`, shape: `[3, 4]`                                            |\n|---------------------------------------------------------------------------------|\n| ![An 3x4 grid, with values in only two of the cells.](images/tensor/sparse.png) |\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sparse tensors store values by index in a memory-efficient manner\nsparse_tensor <- tf$sparse$SparseTensor(\n  indices = rbind(c(0L, 0L),\n                  c(1L, 2L)),\n  values = c(1, 2),\n  dense_shape = as.integer(c(3, 4))\n)\n\nsparse_tensor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSparseTensor(indices=tf.Tensor(\n[[0 0]\n [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2), dtype=int64))\n```\n:::\n\n```{.r .cell-code}\n# You can convert sparse tensors to dense\ntf$sparse$to_dense(sparse_tensor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 0. 0. 0.]\n [0. 0. 2. 0.]\n [0. 0. 0. 0.]], shape=(3, 4), dtype=float32)\n```\n:::\n:::\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.11.0 (~/.virtualenvs/r-tensorflow-website/lib/python3.10/site-packages/tensorflow)\nPython v3.10 (~/.virtualenvs/r-tensorflow-website/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.5 LTS\n\nMatrix products: default\nBLAS:   /home/tomasz/opt/R-4.2.1/lib/R/lib/libRblas.so\nLAPACK: /usr/lib/x86_64-linux-gnu/libmkl_intel_lp64.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] tensorflow_2.9.0.9000\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9           whisker_0.4.1        knitr_1.41          \n [4] magrittr_2.0.3       here_1.0.1           lattice_0.20-45     \n [7] rlang_1.0.6          fastmap_1.1.0        fansi_1.0.3         \n[10] stringr_1.5.0        tools_4.2.1          grid_4.2.1          \n[13] xfun_0.35            png_0.1-8            utf8_1.2.2          \n[16] cli_3.4.1            tfruns_1.5.1         htmltools_0.5.4     \n[19] rprojroot_2.0.3      yaml_2.3.6           digest_0.6.31       \n[22] tibble_3.1.8         lifecycle_1.0.3      Matrix_1.5-3        \n[25] base64enc_0.1-3      htmlwidgets_1.5.4    vctrs_0.5.1         \n[28] glue_1.6.2           evaluate_0.18        rmarkdown_2.18      \n[31] stringi_1.7.8        compiler_4.2.1       pillar_1.8.1        \n[34] reticulate_1.26-9000 jsonlite_1.8.4       pkgconfig_2.0.3     \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\"-m pip freeze\"), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.3.0\nasttokens==2.2.1\nastunparse==1.6.3\nbackcall==0.2.0\ncachetools==5.2.0\ncertifi==2022.12.7\ncharset-normalizer==2.1.1\ndecorator==5.1.1\ndill==0.3.6\netils==0.9.0\nexecuting==1.2.0\nflatbuffers==22.12.6\ngast==0.4.0\ngoogle-auth==2.15.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.57.0\ngrpcio==1.51.1\nh5py==3.7.0\nidna==3.4\nimportlib-resources==5.10.1\nipython==8.7.0\njedi==0.18.2\nkaggle==1.5.12\nkeras==2.11.0\nkeras-tuner==1.1.3\nkt-legacy==1.0.4\nlibclang==14.0.6\nMarkdown==3.4.1\nMarkupSafe==2.1.1\nmatplotlib-inline==0.1.6\nnumpy==1.23.5\noauthlib==3.2.2\nopt-einsum==3.3.0\npackaging==22.0\npandas==1.5.2\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.3.0\npromise==2.3\nprompt-toolkit==3.0.36\nprotobuf==3.19.6\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\npydot==1.4.2\nPygments==2.13.0\npyparsing==3.0.9\npython-dateutil==2.8.2\npython-slugify==7.0.0\npytz==2022.6\nPyYAML==6.0\nrequests==2.28.1\nrequests-oauthlib==1.3.1\nrsa==4.9\nscipy==1.9.3\nsix==1.16.0\nstack-data==0.6.2\ntensorboard==2.11.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.11.0\ntensorflow-datasets==4.7.0\ntensorflow-estimator==2.11.0\ntensorflow-hub==0.12.0\ntensorflow-io-gcs-filesystem==0.28.0\ntensorflow-metadata==1.12.0\ntermcolor==2.1.1\ntext-unidecode==1.3\ntoml==0.10.2\ntqdm==4.64.1\ntraitlets==5.7.1\ntyping_extensions==4.4.0\nurllib3==1.26.13\nwcwidth==0.2.5\nWerkzeug==2.2.2\nwrapt==1.14.1\nzipp==3.11.0\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2022-12-16 \nPage render time: 5 seconds\n```\n:::\n:::\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}