{
  "hash": "39192ce3d046a287f88cb93b29abc0ab",
  "result": {
    "markdown": "---\ntitle: \"Transfer learning and fine-tuning\"\ndescription: Guide to taking pre-trained models and adpating them to new, similar tasks.\n---\n\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nprintf <- function(...) writeLines(sprintf(...))\n```\n:::\n\n\n## Introduction\n\n**Transfer learning** consists of taking features learned on one\nproblem, and leveraging them on a new, similar problem. For instance,\nfeatures from a model that has learned to identify racoons may be useful\nto kick-start a model meant to identify skunks.\n\nTransfer learning is usually done for tasks where your dataset has too\nlittle data to train a full-scale model from scratch.\n\nThe most common incarnation of transfer learning in the context of deep\nlearning is the following workflow:\n\n1.  Take layers from a previously trained model.\n2.  Freeze them, so as to avoid destroying any of the information they\n    contain during future training rounds.\n3.  Add some new, trainable layers on top of the frozen layers. They\n    will learn to turn the old features into predictions on a new\n    dataset.\n4.  Train the new layers on your dataset.\n\nA last, optional step, is **fine-tuning**, which consists of unfreezing\nthe entire model you obtained above (or part of it), and re-training it\non the new data with a very low learning rate. This can potentially\nachieve meaningful improvements, by incrementally adapting the\npretrained features to the new data.\n\nFirst, we will go over the Keras `trainable` API in detail, which\nunderlies most transfer learning and fine-tuning workflows.\n\nThen, we'll demonstrate the typical workflow by taking a model\npretrained on the ImageNet dataset, and retraining it on the Kaggle\n\"cats vs dogs\" classification dataset.\n\nThis is adapted from [Deep Learning with\nR](https://www.manning.com/books/deep-learning-with-r) and the 2016 blog\npost [\"building powerful image classification models using very little\ndata\"](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html).\n\n## Freezing layers: understanding the `trainable` attribute\n\nLayers and models have three weight attributes:\n\n-   `weights` is the list of all weights variables of the layer.\n-   `trainable_weights` is the list of those that are meant to be\n    updated (via gradient descent) to minimize the loss during training.\n-   `non_trainable_weights` is the list of those that aren't meant to be\n    trained. Typically they are updated by the model during the forward\n    pass.\n\n**Example: the `Dense` layer has 2 trainable weights (kernel and bias)**\n\n\n::: {.cell hold='true'}\n\n```{.r .cell-code}\nlayer <- layer_dense(units = 3)\nlayer$build(shape(NULL, 4))\n\nprintf(\"weights: %s\", length(layer$weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nweights: 2\n```\n:::\n\n```{.r .cell-code}\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrainable_weights: 2\n```\n:::\n\n```{.r .cell-code}\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnon_trainable_weights: 0\n```\n:::\n:::\n\n\nIn general, all weights are trainable weights. The only built-in layer\nthat has non-trainable weights is `layer_batch_normalization()`. It uses\nnon-trainable weights to keep track of the mean and variance of its\ninputs during training. To learn how to use non-trainable weights in\nyour own custom layers, see the [guide to writing new layers from\nscratch](https://keras.rstudio.com/articles/new-guides/making_new_layers_and_models_via_subclassing.html).\n\n**Example: The layer instance returned by `layer_batch_normalization()`\nhas 2 trainable weights and 2 non-trainable weights**\n\n\n::: {.cell hold='true'}\n\n```{.r .cell-code}\nlayer <- layer_batch_normalization()\nlayer$build(shape(NULL, 4))\n\nprintf(\"weights: %s\", length(layer$weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nweights: 4\n```\n:::\n\n```{.r .cell-code}\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrainable_weights: 2\n```\n:::\n\n```{.r .cell-code}\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnon_trainable_weights: 2\n```\n:::\n:::\n\n\nLayers and models also feature a boolean attribute `trainable`. Its\nvalue can be changed. Setting `layer$trainable` to `FALSE` moves all the\nlayer's weights from trainable to non-trainable. This is called\n\"freezing\" the layer: the state of a frozen layer won't be updated\nduring training (either when training with `fit()` or when training with\nany custom loop that relies on `trainable_weights` to apply gradient\nupdates).\n\n**Example: setting `trainable` to `False`**\n\n\n::: {.cell hold='true'}\n\n```{.r .cell-code}\nlayer = layer_dense(units = 3)\nlayer$build(shape(NULL, 4))  # Create the weights\nlayer$trainable <- FALSE     # Freeze the layer\n\nprintf(\"weights: %s\", length(layer$weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nweights: 2\n```\n:::\n\n```{.r .cell-code}\nprintf(\"trainable_weights: %s\", length(layer$trainable_weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrainable_weights: 0\n```\n:::\n\n```{.r .cell-code}\nprintf(\"non_trainable_weights: %s\", length(layer$non_trainable_weights))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnon_trainable_weights: 2\n```\n:::\n:::\n\n\nWhen a trainable weight becomes non-trainable, its value is no longer\nupdated during training.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make a model with 2 layers\nlayer1 <- layer_dense(units = 3, activation = \"relu\")\nlayer2 <- layer_dense(units = 3, activation = \"sigmoid\")\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  layer1() %>%\n  layer2()\n\n# Freeze the first layer\nlayer1$trainable <- FALSE\n\n# Keep a copy of the weights of layer1 for later reference\ninitial_layer1_weights_values <- get_weights(layer1)\n\n# Train the model\nmodel %>% compile(optimizer = \"adam\", loss = \"mse\")\nmodel %>% fit(k_random_normal(c(2, 3)), k_random_normal(c(2, 3)))\n\n# Check that the weights of layer1 have not changed during training\nfinal_layer1_weights_values <- get_weights(layer1)\nstopifnot(all.equal(initial_layer1_weights_values, final_layer1_weights_values))\n```\n:::\n\n\nDo not confuse the `layer$trainable` attribute with the `training`\nargument in a layer instance's `call` signature `layer(training =)`\n(which controls whether the layer should run its forward pass in\ninference mode or training mode). For more information, see the [Keras\nFAQ](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute).\n\n## Recursive setting of the `trainable` attribute\n\nIf you set `trainable = FALSE` on a model or on any layer that has\nsublayers, all child layers become non-trainable as well.\n\n**Example:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninner_model <- keras_model_sequential(input_shape = c(3)) %>%\n  layer_dense(3, activation = \"relu\") %>%\n  layer_dense(3, activation = \"relu\")\n\nmodel <- keras_model_sequential(input_shape = c(3)) %>%\n  inner_model() %>%\n  layer_dense(3, activation = \"sigmoid\")\n\n\nmodel$trainable <- FALSE  # Freeze the outer model\n\nstopifnot(inner_model$trainable == FALSE)             # All layers in `model` are now frozen\nstopifnot(inner_model$layers[[1]]$trainable == FALSE)  # `trainable` is propagated recursively\n```\n:::\n\n\n## The typical transfer-learning workflow\n\nThis leads us to how a typical transfer learning workflow can be\nimplemented in Keras:\n\n1.  Instantiate a base model and load pre-trained weights into it.\n2.  Freeze all layers in the base model by setting `trainable = FALSE`.\n3.  Create a new model on top of the output of one (or several) layers\n    from the base model.\n4.  Train your new model on your new dataset.\n\nNote that an alternative, more lightweight workflow could also be:\n\n1.  Instantiate a base model and load pre-trained weights into it.\n2.  Run your new dataset through it and record the output of one (or\n    several) layers from the base model. This is called **feature\n    extraction**.\n3.  Use that output as input data for a new, smaller model.\n\nA key advantage of that second workflow is that you only run the base\nmodel once on your data, rather than once per epoch of training. So it's\na lot faster and cheaper.\n\nAn issue with that second workflow, though, is that it doesn't allow you\nto dynamically modify the input data of your new model during training,\nwhich is required when doing data augmentation, for instance. Transfer\nlearning is typically used for tasks when your new dataset has too\nlittle data to train a full-scale model from scratch, and in such\nscenarios data augmentation is very important. So in what follows, we\nwill focus on the first workflow.\n\nHere's what the first workflow looks like in Keras:\n\nFirst, instantiate a base model with pre-trained weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_model <- application_xception(\n  weights = 'imagenet', # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n```\n:::\n\n\nThen, freeze the base model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_model$trainable <- FALSE\n```\n:::\n\n\nCreate a new model on top.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninputs <- layer_input(c(150, 150, 3))\n\noutputs <- inputs %>%\n  # We make sure that the base_model is running in inference mode here,\n  # by passing `training=FALSE`. This is important for fine-tuning, as you will\n  # learn in a few paragraphs.\n  base_model(training=FALSE) %>%\n\n  # Convert features of shape `base_model$output_shape[-1]` to vectors\n  layer_global_average_pooling_2d() %>%\n\n  # A Dense classifier with a single unit (binary classification)\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\n```\n:::\n\n\nTrain the model on new data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>%\n  compile(optimizer = optimizer_adam(),\n          loss = loss_binary_crossentropy(from_logits = TRUE),\n          metrics = metric_binary_accuracy()) %>%\n  fit(new_dataset, epochs = 20, callbacks = ..., validation_data = ...)\n```\n:::\n\n\n## Fine-tuning\n\nOnce your model has converged on the new data, you can try to unfreeze\nall or part of the base model and retrain the whole model end-to-end\nwith a very low learning rate.\n\nThis is an optional last step that can potentially give you incremental\nimprovements. It could also potentially lead to quick overfitting --\nkeep that in mind.\n\nIt is critical to only do this step *after* the model with frozen layers\nhas been trained to convergence. If you mix randomly-initialized\ntrainable layers with trainable layers that hold pre-trained features,\nthe randomly-initialized layers will cause very large gradient updates\nduring training, which will destroy your pre-trained features.\n\nIt's also critical to use a very low learning rate at this stage,\nbecause you are training a much larger model than in the first round of\ntraining, on a dataset that is typically very small. As a result, you\nare at risk of overfitting very quickly if you apply large weight\nupdates. Here, you only want to re-adapt the pretrained weights in an\nincremental way.\n\nThis is how to implement fine-tuning of the whole base model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unfreeze the base model\nbase_model$trainable <- TRUE\n\n# It's important to recompile your model after you make any changes\n# to the `trainable` attribute of any inner layer, so that your changes\n# are taken into account\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5), # Very low learning rate\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\n# Train end-to-end. Be careful to stop before you overfit!\nmodel %>% fit(new_dataset, epochs=10, callbacks=..., validation_data=...)\n```\n:::\n\n\n**Important note about `compile()` and `trainable`**\n\nCalling `compile()` on a model is meant to \"freeze\" the behavior of that\nmodel. This implies that the `trainable` attribute values at the time\nthe model is compiled should be preserved throughout the lifetime of\nthat model, until `compile` is called again. Hence, if you change any\n`trainable` value, make sure to call `compile()` again on your model for\nyour changes to be taken into account.\n\n**Important notes about `layer_batch_normalization()`**\n\nMany image models contain `BatchNormalization` layers. That layer is a\nspecial case on every imaginable count. Here are a few things to keep in\nmind.\n\n-   `BatchNormalization` contains 2 non-trainable weights that get\n    updated during training. These are the variables tracking the mean\n    and variance of the inputs.\n-   When you set `bn_layer$trainable = FALSE`, the `BatchNormalization`\n    layer will run in inference mode, and will not update its mean and\n    variance statistics. This is not the case for other layers in\n    general, as [weight trainability and inference/training modes are\n    two orthogonal\n    concepts](https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute).\n    But the two are tied in the case of the `BatchNormalization` layer.\n-   When you unfreeze a model that contains `BatchNormalization` layers\n    in order to do fine-tuning, you should keep the `BatchNormalization`\n    layers in inference mode by passing `training = FALSE` when calling\n    the base model. Otherwise the updates applied to the non-trainable\n    weights will suddenly destroy what the model has learned.\n\nYou'll see this pattern in action in the end-to-end example at the end\nof this guide.\n\n## Transfer learning and fine-tuning with a custom training loop\n\nIf instead of `fit()`, you are using your own low-level training loop,\nthe workflow stays essentially the same. You should be careful to only\ntake into account the list `model$trainable_weights` when applying\ngradient updates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create base model\nbase_model = application_xception(\n  weights = 'imagenet',\n  input_shape = c(150, 150, 3),\n  include_top = FALSE\n)\n\n# Freeze base model\nbase_model$trainable = FALSE\n\n# Create new model on top.\ninputs <- layer_input(shape = c(150, 150, 3))\noutputs <- inputs %>%\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dense(1)\nmodel <- keras_model(inputs, outputs)\n\nloss_fn <- loss_binary_crossentropy(from_logits = TRUE)\noptimizer <- optimizer_adam()\n\n# helper to zip gradients with weights\nxyz <- function(...) .mapply(c, list(...), NULL)\n\n# Iterate over the batches of a dataset.\nlibrary(tfdatasets)\nnew_dataset <- ...\n\nwhile(!is.null(batch <- iter_next(new_dataset))) {\n  c(inputs, targets) %<-% batch\n  # Open a GradientTape.\n  with(tf$GradientTape() %as% tape, {\n    # Forward pass.\n    predictions = model(inputs)\n    # Compute the loss value for this batch.\n    loss_value = loss_fn(targets, predictions)\n  })\n  # Get gradients of loss w.r.t. the *trainable* weights.\n  gradients <- tape$gradient(loss_value, model$trainable_weights)\n  # Update the weights of the model.\n  optimizer$apply_gradients(xyz(gradients, model$trainable_weights))\n}\n```\n:::\n\n\nLikewise for fine-tuning.\n\n## An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset\n\nTo solidify these concepts, let's walk you through a concrete end-to-end\ntransfer learning and fine-tuning example. We will load the Xception\nmodel, pre-trained on ImageNet, and use it on the Kaggle \"cats vs. dogs\"\nclassification dataset.\n\n### Getting the data\n\nFirst, let's fetch the cats vs. dogs dataset using TFDS. If you have\nyour own dataset, you'll probably want to use the utility\n`image_dataset_from_directory()` to generate similar labeled dataset\nobjects from a set of images on disk filed into class-specific folders.\n\nTransfer learning is most useful when working with very small datasets.\nTo keep our dataset small, we will use 40% of the original training data\n(25,000 images) for training, 10% for validation, and 10% for testing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# reticulate::py_install(\"tensorflow_datasets\", pip = TRUE)\ntfds <- reticulate::import(\"tensorflow_datasets\")\n\nc(train_ds, validation_ds, test_ds) %<-% tfds$load(\n    \"cats_vs_dogs\",\n    # Reserve 10% for validation and 10% for test\n    split = c(\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"),\n    as_supervised=TRUE  # Include labels\n)\n\nprintf(\"Number of training samples: %d\", length(train_ds))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of training samples: 9305\n```\n:::\n\n```{.r .cell-code}\nprintf(\"Number of validation samples: %d\", length(validation_ds) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of validation samples: 2326\n```\n:::\n\n```{.r .cell-code}\nprintf(\"Number of test samples: %d\", length(test_ds))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of test samples: 2326\n```\n:::\n:::\n\n\nThese are the first 9 images in the training dataset -- as you can see,\nthey're all different sizes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tfdatasets)\n\npar(mfrow = c(3, 3), mar = c(1,0,1.5,0))\ntrain_ds %>%\n  dataset_take(9) %>%\n  as_array_iterator() %>%\n  iterate(function(batch) {\n    c(image, label) %<-% batch\n    plot(as.raster(image, max = 255))\n    title(sprintf(\"label: %s   size: %s\",\n                  label, paste(dim(image), collapse = \" x \")))\n  })\n```\n\n::: {.cell-output-display}\n![](transfer_learning_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nWe can also see that label 1 is \"dog\" and label 0 is \"cat\".\n\n### Standardizing the data\n\nOur raw images have a variety of sizes. In addition, each pixel consists\nof 3 integer values between 0 and 255 (RGB level values). This isn't a\ngreat fit for feeding a neural network. We need to do 2 things:\n\n-   Standardize to a fixed image size. We pick 150x150.\n-   Normalize pixel values between -1 and 1. We'll do this using a\n    `layer_normalization()` as part of the model itself.\n\nIn general, it's a good practice to develop models that take raw data as\ninput, as opposed to models that take already-preprocessed data. The\nreason being that, if your model expects preprocessed data, any time you\nexport your model to use it elsewhere (in a web browser, in a mobile\napp), you'll need to reimplement the exact same preprocessing pipeline.\nThis gets very tricky very quickly. So we should do the least possible\namount of preprocessing before hitting the model.\n\nHere, we'll do image resizing in the data pipeline (because a deep\nneural network can only process contiguous batches of data), and we'll\ndo the input value scaling as part of the model, when we create it.\n\nLet's resize images to 150x150:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr, include.only = \"%<>%\")\nsize <- as.integer(c(150, 150))\ntrain_ds      %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\nvalidation_ds %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\ntest_ds       %<>% dataset_map(function(x, y) list(tf$image$resize(x, size), y))\n```\n:::\n\n\nBesides, let's batch the data and use caching and prefetching to\noptimize loading speed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_cache_batch_prefetch <- function(dataset, batch_size = 32, buffer_size = 10) {\n  dataset %>%\n    dataset_cache() %>%\n    dataset_batch(batch_size) %>%\n    dataset_prefetch(buffer_size)\n}\n\ntrain_ds      %<>% dataset_cache_batch_prefetch()\nvalidation_ds %<>% dataset_cache_batch_prefetch()\ntest_ds       %<>% dataset_cache_batch_prefetch()\n```\n:::\n\n\n### Using random data augmentation\n\nWhen you don't have a large image dataset, it's a good practice to\nartificially introduce sample diversity by applying random yet realistic\ntransformations to the training images, such as random horizontal\nflipping or small random rotations. This helps expose the model to\ndifferent aspects of the training data while slowing down overfitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_augmentation <- keras_model_sequential() %>%\n  layer_random_flip(\"horizontal\") %>%\n  layer_random_rotation(.1)\n```\n:::\n\n\nLet's visualize what the first image of the first batch looks like after\nvarious random transformations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbatch <- train_ds %>%\n  dataset_take(1) %>%\n  as_iterator() %>% iter_next()\n\nc(images, labels) %<-% batch\nfirst_image <- images[1, all_dims(), drop = FALSE]\naugmented_image <- data_augmentation(first_image, training = TRUE)\n\nplot_image <- function(image, main = deparse1(substitute(image))) {\n  image %>%\n    k_squeeze(1) %>% # drop batch dim\n    as.array() %>%   # convert from tensor to R array\n    as.raster(max = 255) %>%\n    plot()\n\n  if(!is.null(main))\n    title(main)\n}\n\npar(mfrow = c(2, 2), mar = c(1, 1, 1.5, 1))\nplot_image(first_image)\nplot_image(augmented_image)\nplot_image(data_augmentation(first_image, training = TRUE), \"augmented 2\")\nplot_image(data_augmentation(first_image, training = TRUE), \"augmented 3\")\n```\n\n::: {.cell-output-display}\n![](transfer_learning_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n## Build a model\n\nNow let's build a model that follows the blueprint we've explained\nearlier.\n\nNote that:\n\n-   We add `layer_rescaling()` to scale input values (initially in the\n    `[0, 255]` range) to the `[-1, 1]` range.\n-   We add a `layer_dropout()` before the classification layer, for\n    regularization.\n-   We make sure to pass `training = FALSE` when calling the base model,\n    so that it runs in inference mode, so that batchnorm statistics\n    don't get updated even after we unfreeze the base model for\n    fine-tuning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_model = application_xception(\n  weights = \"imagenet\", # Load weights pre-trained on ImageNet.\n  input_shape = c(150, 150, 3),\n  include_top = FALSE # Do not include the ImageNet classifier at the top.\n)\n\n# Freeze the base_model\nbase_model$trainable <- FALSE\n\n# Create new model on top\ninputs <- layer_input(shape = c(150, 150, 3))\n\noutputs <- inputs %>%\n  data_augmentation() %>%   # Apply random data augmentation\n\n  # Pre-trained Xception weights requires that input be scaled\n  # from (0, 255) to a range of (-1., +1.), the rescaling layer\n  # outputs: `(inputs * scale) + offset`\n  layer_rescaling(scale = 1 / 127.5, offset = -1) %>%\n\n  # The base model contains batchnorm layers. We want to keep them in inference mode\n  # when we unfreeze the base model for fine-tuning, so we make sure that the\n  # base_model is running in inference mode here.\n  base_model(training = FALSE) %>%\n  layer_global_average_pooling_2d() %>%\n  layer_dropout(.2) %>%\n  layer_dense(1)\n\nmodel <- keras_model(inputs, outputs)\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"model_1\"\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n input_7 (InputLayer)        [(None, 150, 150, 3)]     0         Y          \n sequential_3 (Sequential)   (None, 150, 150, 3)       0         Y          \n rescaling (Rescaling)       (None, 150, 150, 3)       0         Y          \n xception (Functional)       (None, 5, 5, 2048)        20861480  N          \n global_average_pooling2d_1   (None, 2048)             0         Y          \n (GlobalAveragePooling2D)                                                   \n dropout (Dropout)           (None, 2048)              0         Y          \n dense_8 (Dense)             (None, 1)                 2049      Y          \n============================================================================\nTotal params: 20,863,529\nTrainable params: 2,049\nNon-trainable params: 20,861,480\n____________________________________________________________________________\n```\n:::\n:::\n\n\n## Train the top layer\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = optimizer_adam(),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 2\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)\n```\n:::\n\n\n## Do a round of fine-tuning of the entire model\n\nFinally, let's unfreeze the base model and train the entire model\nend-to-end with a low learning rate.\n\nImportantly, although the base model becomes trainable, it is still\nrunning in inference mode since we passed `training = FALSE` when\ncalling it when we built the model. This means that the batch\nnormalization layers inside won't update their batch statistics. If they\ndid, they would wreck havoc on the representations learned by the model\nso far.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unfreeze the base_model. Note that it keeps running in inference mode\n# since we passed `training = FALSE` when calling it. This means that\n# the batchnorm layers will not update their batch statistics.\n# This prevents the batchnorm layers from undoing all the training\n# we've done so far.\nbase_model$trainable <- TRUE\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"model_1\"\n____________________________________________________________________________\n Layer (type)                Output Shape              Param #   Trainable  \n============================================================================\n input_7 (InputLayer)        [(None, 150, 150, 3)]     0         Y          \n sequential_3 (Sequential)   (None, 150, 150, 3)       0         Y          \n rescaling (Rescaling)       (None, 150, 150, 3)       0         Y          \n xception (Functional)       (None, 5, 5, 2048)        20861480  Y          \n global_average_pooling2d_1   (None, 2048)             0         Y          \n (GlobalAveragePooling2D)                                                   \n dropout (Dropout)           (None, 2048)              0         Y          \n dense_8 (Dense)             (None, 1)                 2049      Y          \n============================================================================\nTotal params: 20,863,529\nTrainable params: 20,809,001\nNon-trainable params: 54,528\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-5),\n  loss = loss_binary_crossentropy(from_logits = TRUE),\n  metrics = metric_binary_accuracy()\n)\n\nepochs <- 1\nmodel %>% fit(train_ds, epochs = epochs, validation_data = validation_ds)\n```\n:::\n\n\nAfter 10 epochs, fine-tuning gains us a nice improvement here.\n\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.11.0 (~/.virtualenvs/r-tensorflow-website/lib/python3.10/site-packages/tensorflow)\nPython v3.10 (~/.virtualenvs/r-tensorflow-website/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.5 LTS\n\nMatrix products: default\nBLAS:   /home/tomasz/opt/R-4.2.1/lib/R/lib/libRblas.so\nLAPACK: /usr/lib/x86_64-linux-gnu/libmkl_intel_lp64.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] magrittr_2.0.3        tfdatasets_2.9.0.9000 keras_2.9.0.9000     \n[4] tensorflow_2.9.0.9000\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9           pillar_1.8.1         compiler_4.2.1      \n [4] base64enc_0.1-3      tools_4.2.1          zeallot_0.1.0       \n [7] digest_0.6.31        jsonlite_1.8.4       evaluate_0.18       \n[10] lifecycle_1.0.3      tibble_3.1.8         lattice_0.20-45     \n[13] pkgconfig_2.0.3      png_0.1-8            rlang_1.0.6         \n[16] Matrix_1.5-3         cli_3.4.1            yaml_2.3.6          \n[19] xfun_0.35            fastmap_1.1.0        stringr_1.5.0       \n[22] knitr_1.41           generics_0.1.3       vctrs_0.5.1         \n[25] htmlwidgets_1.5.4    tidyselect_1.2.0     rprojroot_2.0.3     \n[28] grid_4.2.1           reticulate_1.26-9000 glue_1.6.2          \n[31] here_1.0.1           R6_2.5.1             fansi_1.0.3         \n[34] rmarkdown_2.18       whisker_0.4.1        htmltools_0.5.4     \n[37] tfruns_1.5.1         utf8_1.2.2           stringi_1.7.8       \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\"-m pip freeze\"), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.3.0\nasttokens==2.2.1\nastunparse==1.6.3\nbackcall==0.2.0\ncachetools==5.2.0\ncertifi==2022.12.7\ncharset-normalizer==2.1.1\ndecorator==5.1.1\ndill==0.3.6\netils==0.9.0\nexecuting==1.2.0\nflatbuffers==22.12.6\ngast==0.4.0\ngoogle-auth==2.15.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.57.0\ngrpcio==1.51.1\nh5py==3.7.0\nidna==3.4\nimportlib-resources==5.10.1\nipython==8.7.0\njedi==0.18.2\nkaggle==1.5.12\nkeras==2.11.0\nkeras-tuner==1.1.3\nkt-legacy==1.0.4\nlibclang==14.0.6\nMarkdown==3.4.1\nMarkupSafe==2.1.1\nmatplotlib-inline==0.1.6\nnumpy==1.23.5\noauthlib==3.2.2\nopt-einsum==3.3.0\npackaging==22.0\npandas==1.5.2\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.3.0\npromise==2.3\nprompt-toolkit==3.0.36\nprotobuf==3.19.6\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\npydot==1.4.2\nPygments==2.13.0\npyparsing==3.0.9\npython-dateutil==2.8.2\npython-slugify==7.0.0\npytz==2022.6\nPyYAML==6.0\nrequests==2.28.1\nrequests-oauthlib==1.3.1\nrsa==4.9\nscipy==1.9.3\nsix==1.16.0\nstack-data==0.6.2\ntensorboard==2.11.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.11.0\ntensorflow-datasets==4.7.0\ntensorflow-estimator==2.11.0\ntensorflow-hub==0.12.0\ntensorflow-io-gcs-filesystem==0.28.0\ntensorflow-metadata==1.12.0\ntermcolor==2.1.1\ntext-unidecode==1.3\ntoml==0.10.2\ntqdm==4.64.1\ntraitlets==5.7.1\ntyping_extensions==4.4.0\nurllib3==1.26.13\nwcwidth==0.2.5\nWerkzeug==2.2.2\nwrapt==1.14.1\nzipp==3.11.0\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2022-12-16 \nPage render time: 1 minutes and 48 seconds\n```\n:::\n:::\n:::\n\n",
    "supporting": [
      "transfer_learning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}