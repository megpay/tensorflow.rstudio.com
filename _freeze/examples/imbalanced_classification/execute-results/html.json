{
  "hash": "20bd8b4f88baffeaea853d026128aecc",
  "result": {
    "markdown": "---\ntitle: \"Imbalanced classification: credit card fraud detection\"\nauthors:\n  - \"[fchollet](https://twitter.com/fchollet)\"\n  - \"[terrytangyuan](https://github.com/terrytangyuan) - R adaptation\"\n  - \"[t-kalinowski](https://github.com/t-kalinowski) - R adaptation\"\ndate-created: 2022/11/22\ndate-last-modified: 2022/11/22\ndescription: Demonstration of how to handle highly imbalanced classification problems.\ncategories: [structured]\naliases:\n  - ../guide/keras/examples/imbalanced_classification/index.html\n---\n\n\n## Introduction\n\nThis example looks at the [Kaggle Credit Card Fraud\nDetection](https://www.kaggle.com/mlg-ulb/creditcardfraud/) dataset to\ndemonstrate how to train a classification model on data with highly\nimbalanced classes. You can download the data by clicking \"Download\" at\nthe link, or if you're setup with a kaggle API key at\n`\"~/.kaggle/kagle.json\"`, you can run the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreticulate::py_install(\"kaggle\", pip = TRUE)\nsystem(\"kaggle datasets download -d mlg-ulb/creditcardfraud\")\nzip::unzip(\"creditcardfraud.zip\", files = \"creditcard.csv\")\n```\n:::\n\n\n## First, read in the CSV data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset.seed(1234)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- readr::read_csv(\"creditcard.csv\")\ntibble::glimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 284,807\nColumns: 31\n$ Time   <dbl> 0, 0, 1, 1, 2, 2, 4, 7, 7, 9, 10, 10, 10, 11, 12, 12, 12, 1…\n$ V1     <dbl> -1.3598071, 1.1918571, -1.3583541, -0.9662717, -1.1582331, …\n$ V2     <dbl> -0.07278117, 0.26615071, -1.34016307, -0.18522601, 0.877736…\n$ V3     <dbl> 2.53634674, 0.16648011, 1.77320934, 1.79299334, 1.54871785,…\n$ V4     <dbl> 1.37815522, 0.44815408, 0.37977959, -0.86329128, 0.40303393…\n$ V5     <dbl> -0.33832077, 0.06001765, -0.50319813, -0.01030888, -0.40719…\n$ V6     <dbl> 0.46238778, -0.08236081, 1.80049938, 1.24720317, 0.09592146…\n$ V7     <dbl> 0.239598554, -0.078802983, 0.791460956, 0.237608940, 0.5929…\n$ V8     <dbl> 0.098697901, 0.085101655, 0.247675787, 0.377435875, -0.2705…\n$ V9     <dbl> 0.3637870, -0.2554251, -1.5146543, -1.3870241, 0.8177393, -…\n$ V10    <dbl> 0.09079417, -0.16697441, 0.20764287, -0.05495192, 0.7530744…\n$ V11    <dbl> -0.55159953, 1.61272666, 0.62450146, -0.22648726, -0.822842…\n$ V12    <dbl> -0.61780086, 1.06523531, 0.06608369, 0.17822823, 0.53819555…\n$ V13    <dbl> -0.99138985, 0.48909502, 0.71729273, 0.50775687, 1.34585159…\n$ V14    <dbl> -0.31116935, -0.14377230, -0.16594592, -0.28792375, -1.1196…\n$ V15    <dbl> 1.468176972, 0.635558093, 2.345864949, -0.631418118, 0.1751…\n$ V16    <dbl> -0.47040053, 0.46391704, -2.89008319, -1.05964725, -0.45144…\n$ V17    <dbl> 0.207971242, -0.114804663, 1.109969379, -0.684092786, -0.23…\n$ V18    <dbl> 0.02579058, -0.18336127, -0.12135931, 1.96577500, -0.038194…\n$ V19    <dbl> 0.40399296, -0.14578304, -2.26185710, -1.23262197, 0.803486…\n$ V20    <dbl> 0.25141210, -0.06908314, 0.52497973, -0.20803778, 0.4085423…\n$ V21    <dbl> -0.018306778, -0.225775248, 0.247998153, -0.108300452, -0.0…\n$ V22    <dbl> 0.277837576, -0.638671953, 0.771679402, 0.005273597, 0.7982…\n$ V23    <dbl> -0.110473910, 0.101288021, 0.909412262, -0.190320519, -0.13…\n$ V24    <dbl> 0.06692807, -0.33984648, -0.68928096, -1.17557533, 0.141266…\n$ V25    <dbl> 0.12853936, 0.16717040, -0.32764183, 0.64737603, -0.2060095…\n$ V26    <dbl> -0.18911484, 0.12589453, -0.13909657, -0.22192884, 0.502292…\n$ V27    <dbl> 0.133558377, -0.008983099, -0.055352794, 0.062722849, 0.219…\n$ V28    <dbl> -0.021053053, 0.014724169, -0.059751841, 0.061457629, 0.215…\n$ Amount <dbl> 149.62, 2.69, 378.66, 123.50, 69.99, 3.67, 4.99, 40.80, 93.…\n$ Class  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n```\n:::\n:::\n\n\n## Prepare a validation set\n\n\n::: {.cell}\n\n```{.r .cell-code}\nval_idxs <- nrow(df) %>% sample.int(., ceiling( . * 0.2))\nval_df <- df[val_idxs, ]\ntrain_df <- df[-val_idxs, ]\n\nsprintf(\"Number of training samples: %s\", nrow(train_df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Number of training samples: 227845\"\n```\n:::\n\n```{.r .cell-code}\nsprintf(\"Number of validation samples: %s\", nrow(val_df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Number of validation samples: 56962\"\n```\n:::\n:::\n\n\n## Analyze class imbalance in the targets\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(train_df$Class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n     0      1 \n227450    395 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_df$Class %>% {\n  cat(sprintf(\n    \"Number of positive samples in training data: %s (%.2f%% of total)\\n\",\n    sum(.), 100 * mean(.)))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of positive samples in training data: 395 (0.17% of total)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nweight_for_0 <- 1 / sum(train_df$Class == 0)\nweight_for_1 <- 1 / sum(train_df$Class == 1)\n```\n:::\n\n\n## Normalize the data using training set statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeature_names <- colnames(train_df) %>% setdiff(\"Class\")\n\nmeans <- lapply(train_df[feature_names], mean)\nstds <- lapply(train_df[feature_names], sd)\n\nfor (name in feature_names) {\n  train_df[[name]] %<>% { (. - means[[name]]) / stds[[name]] }\n    val_df[[name]] %<>% { (. - means[[name]]) / stds[[name]] }\n}\n```\n:::\n\n\n## Build a binary classification model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential(input_shape = c(length(feature_names))) %>%\n  layer_dense(256, activation = \"relu\") %>%\n  layer_dense(256, activation = \"relu\") %>%\n  layer_dropout(0.3) %>%\n  layer_dense(256, activation = \"relu\") %>%\n  layer_dropout(0.3) %>%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n____________________________________________________________________________\n Layer (type)                     Output Shape                  Param #     \n============================================================================\n dense_3 (Dense)                  (None, 256)                   7936        \n dense_2 (Dense)                  (None, 256)                   65792       \n dropout_1 (Dropout)              (None, 256)                   0           \n dense_1 (Dense)                  (None, 256)                   65792       \n dropout (Dropout)                (None, 256)                   0           \n dense (Dense)                    (None, 1)                     257         \n============================================================================\nTotal params: 139,777\nTrainable params: 139,777\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n:::\n\n\n## Train the model with `class_weight` argument\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrics <- list(\n  metric_false_negatives(name = \"fn\"),\n  metric_false_positives(name = \"fp\"),\n  metric_true_negatives(name = \"tn\"),\n  metric_true_positives(name = \"tp\"),\n  metric_precision(name = \"precision\"),\n  metric_recall(name = \"recall\")\n)\nmodel %>% compile(\n  optimizer = optimizer_adam(1e-2),\n  loss = \"binary_crossentropy\",\n  metrics = metrics\n)\nclass_weight <- list(\"0\" = weight_for_0,\n                     \"1\" = weight_for_1)\ncallbacks <- list(\n  callback_model_checkpoint(\"fraud_model_at_epoch_{epoch}.h5\"))\n\ntrain_features <- as.matrix(train_df[feature_names])\ntrain_targets <- as.matrix(train_df$Class)\nvalidation_data <- list(\n   as.matrix(val_df[feature_names]),\n   as.matrix(val_df$Class))\n\nmodel %>%\n  fit(train_features, train_targets,\n      validation_data = validation_data,\n      class_weight = class_weight,\n      batch_size = 2048, epochs = 30,\n      callbacks = callbacks,\n      verbose = 2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nval_pred <- model %>%\n  predict(as.matrix(val_df[feature_names])) %>%\n  { ifelse(. > .5, 1, 0) }\n\npred_correct <- val_df$Class == val_pred\ncat(sprintf(\"Validation accuracy: %.2f\", mean(pred_correct)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValidation accuracy: 0.99\n```\n:::\n\n```{.r .cell-code}\nfraudulent <- val_df$Class == 1\n\nn_fraudulent_detected <- sum(fraudulent & pred_correct)\nn_fraudulent_missed <- sum(fraudulent & !pred_correct)\nn_legitimate_flagged <- sum(!fraudulent & !pred_correct)\n```\n:::\n\n\n## Conclusions\n\nAt the end of training, out of\n56,962 validation transactions, we\nare:\n\n-   Correctly identifying\n    85 of them as\n    fraudulent\n-   Missing 12\n    fraudulent transactions\n-   At the cost of incorrectly flagging\n    839 legitimate\n    transactions\n\nIn the real world, one would put an even higher weight on class 1, so as\nto reflect that False Negatives are more costly than False Positives.\n\nNext time your credit card gets declined in an online purchase -- this\nis why.\n\n| Trained Model                                                                                                                                                          | Demo                                                                                                                                                                             |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Imbalanced%20Classification-black.svg)](https://huggingface.co/keras-io/imbalanced_classification) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Imbalanced%20Classification-black.svg)](https://huggingface.co/spaces/keras-io/Credit_Card_Fraud_Detection) |\n\n: Example available on HuggingFace.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}