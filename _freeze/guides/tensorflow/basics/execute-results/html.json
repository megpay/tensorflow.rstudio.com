{
  "hash": "b8eaed0f7e16fc5efba375bf51c11abc",
  "result": {
    "markdown": "---\ntitle: Tensorflow Basics\ndescription: >\n  Start here for a quick overview of *TensorFlow basics*.\n---\n\n\n##### Copyright 2022 The TensorFlow Authors.\n\nThis guide provides a quick overview of *TensorFlow basics*. Each\nsection of this doc is an overview of a larger topic---you can find\nlinks to full guides at the end of each section.\n\nTensorFlow is an end-to-end platform for machine learning. It supports\nthe following:\n\n-   Multidimensional-array based numeric computation (similar to\n    [Numpy](https://numpy.org) or `base::array()` in R.\n\n-   GPU and distributed processing\n\n-   Automatic differentiation\n\n-   Model construction, training, and export\n\n-   And more\n\n## Tensors\n\nTensorFlow operates on multidimensional arrays or *tensors* represented\nas `tensorflow.tensor` objects. Here is a two-dimensional tensor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\n\nx <- as_tensor(1:6, dtype = \"float32\", shape = c(2, 3))\n\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]], shape=(2, 3), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\nx$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([2, 3])\n```\n:::\n\n```{.r .cell-code}\nx$dtype\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.float32\n```\n:::\n:::\n\n\nThe most important attributes of a tensor are its `shape` and `dtype`:\n\n-   `tensor$shape`: tells you the size of the tensor along each of its\n    axes.\n-   `tensor$dtype`: tells you the type of all the elements in the\n    tensor.\n\nTensorFlow implements standard mathematical operations on tensors, as\nwell as many operations specialized for machine learning.\n\nFor example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx + x\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 2.  4.  6.]\n [ 8. 10. 12.]], shape=(2, 3), dtype=float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n5 * x\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[ 5. 10. 15.]\n [20. 25. 30.]], shape=(2, 3), dtype=float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$matmul(x, t(x)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[14. 32.]\n [32. 77.]], shape=(2, 2), dtype=float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$concat(list(x, x, x), axis = 0L)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[1. 2. 3.]\n [4. 5. 6.]\n [1. 2. 3.]\n [4. 5. 6.]\n [1. 2. 3.]\n [4. 5. 6.]], shape=(6, 3), dtype=float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntf$nn$softmax(x, axis = -1L)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(\n[[0.09003057 0.24472848 0.6652409 ]\n [0.09003057 0.24472848 0.6652409 ]], shape=(2, 3), dtype=float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(x) # same as tf$reduce_sum(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(21.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nRunning large calculations on CPU can be slow. When properly configured,\nTensorFlow can use accelerator hardware like GPUs to execute operations\nvery quickly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (length(tf$config$list_physical_devices('GPU')))\n  message(\"TensorFlow **IS** using the GPU\") else\n  message(\"TensorFlow **IS NOT** using the GPU\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTensorFlow **IS** using the GPU\n```\n:::\n:::\n\n\nRefer to the [Tensor guide](tensorflow/guide/tensor.qmd) for details.\n\n## Variables\n\nNormal tensor objects are immutable. To store model weights (or other\nmutable state) in TensorFlow use a `tf$Variable`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar <- tf$Variable(c(0, 0, 0))\nvar\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar$assign(c(1, 2, 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'UnreadVariable' shape=(3,) dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar$assign_add(c(1, 1, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<tf.Variable 'UnreadVariable' shape=(3,) dtype=float32, numpy=array([2., 3., 4.], dtype=float32)>\n```\n:::\n:::\n\n\nRefer to the [Variables guide](variable$ipynb) for details.\n\n## Automatic differentiation\n\n[*Gradient descent*](https://en.wikipedia.org/wiki/Gradient_descent) and\nrelated algorithms are a cornerstone of modern machine learning.\n\nTo enable this, TensorFlow implements automatic differentiation\n(autodiff), which uses calculus to compute gradients. Typically you'll\nuse this to calculate the gradient of a model's *error* or *loss* with\nrespect to its weights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(1.0)\n\nf <- function(x)\n  x^2 + 2*x - 5\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nf(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(-2.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nAt `x = 1.0`, `y = f(x) = (1^2 + 2*1 - 5) = -2`.\n\nThe derivative of `y` is `y' = f'(x) = (2*x + 2) = 4`. TensorFlow can\ncalculate this automatically:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(tf$GradientTape() %as% tape, {\n  y <- f(x)\n})\n\ng_x <- tape$gradient(y, x)  # g(x) = dy/dx\n\ng_x\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(4.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nThis simplified example only takes the derivative with respect to a\nsingle scalar (`x`), but TensorFlow can compute the gradient with\nrespect to any number of non-scalar tensors simultaneously.\n\nRefer to the [Autodiff guide](tensorflow/guide/autodiff.qmd) for\ndetails.\n\n## Graphs and `tf_function`\n\nWhile you can use TensorFlow interactively like any R library,\nTensorFlow also provides tools for:\n\n-   **Performance optimization**: to speed up training and inference.\n-   **Export**: so you can save your model when it's done training.\n\nThese require that you use `tf_function()` to separate your\npure-TensorFlow code from R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_func <- tf_function(function(x) {\n  message('Tracing.')\n  tf$reduce_sum(x)\n})\n```\n:::\n\n\nThe first time you run the `tf_function`, although it executes in R, it\ncaptures a complete, optimized graph representing the TensorFlow\ncomputations done within the function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(1:3)\nmy_func(x)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTracing.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(6, shape=(), dtype=int32)\n```\n:::\n:::\n\n\nOn subsequent calls TensorFlow only executes the optimized graph,\nskipping any non-TensorFlow steps. Below, note that `my_func` doesn't\nprint `\"Tracing.\"` since `message` is an R function, not a TensorFlow\nfunction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(10:8)\nmy_func(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(27, shape=(), dtype=int32)\n```\n:::\n:::\n\n\nA graph may not be reusable for inputs with a different *signature*\n(`shape` and `dtype`), so a new graph is generated instead:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(c(10.0, 9.1, 8.2), dtype=tf$dtypes$float32)\nmy_func(x)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTracing.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(27.3, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nThese captured graphs provide two benefits:\n\n-   In many cases they provide a significant speedup in execution\n    (though not this trivial example).\n-   You can export these graphs, using `tf$saved_model`, to run on other\n    systems like a\n    [server](https://www.tensorflow.org/tfx/serving/docker) or a [mobile\n    device](https://www.tensorflow.org/lite/guide), no Python\n    installation required.\n\nRefer to [Intro to graphs](tensorflow/guide/intro_to_graphs.qmd) for\nmore details.\n\n## Modules, layers, and models\n\n`tf$Module` is a class for managing your `tf$Variable` objects, and the\n`tf_function` objects that operate on them. The `tf$Module` class is\nnecessary to support two significant features:\n\n1.  You can save and restore the values of your variables using\n    `tf$train$Checkpoint`. This is useful during training as it is quick\n    to save and restore a model's state.\n2.  You can import and export the `tf$Variable` values *and* the\n    `tf$function` graphs using `tf$saved_model`. This allows you to run\n    your model independently of the Python program that created it.\n\nHere is a complete example exporting a simple `tf$Module` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras) # %py_class% is exported by the keras package at this time\nMyModule(tf$Module) %py_class% {\n  initialize <- function(self, value) {\n    self$weight <- tf$Variable(value)\n  }\n  \n  multiply <- tf_function(function(self, x) {\n    x * self$weight\n  })\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- MyModule(3)\nmod$multiply(as_tensor(c(1, 2, 3), \"float32\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([3. 6. 9.], shape=(3), dtype=float32)\n```\n:::\n:::\n\n\nSave the `Module`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_path <- tempfile()\ntf$saved_model$save(mod, save_path)\n```\n:::\n\n\nThe resulting SavedModel is independent of the code that created it. You\ncan load a SavedModel from R, Python, other language bindings, or\n[TensorFlow Serving](https://www.tensorflow.org/tfx/serving/docker). You\ncan also convert it to run with [TensorFlow\nLite](https://www.tensorflow.org/lite/guide) or [TensorFlow\nJS](https://www.tensorflow.org/js/guide).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreloaded <- tf$saved_model$load(save_path)\nreloaded$multiply(as_tensor(c(1, 2, 3), \"float32\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([3. 6. 9.], shape=(3), dtype=float32)\n```\n:::\n:::\n\n\nThe `tf$keras$layers$Layer` and `tf$keras$Model` classes build on\n`tf$Module` providing additional functionality and convenience methods\nfor building, training, and saving models. Some of these are\ndemonstrated in the next section.\n\nRefer to [Intro to modules](tensorflow/guide/intro_to_modules.qmd) for\ndetails.\n\n## Training loops\n\nNow put this all together to build a basic model and train it from\nscratch.\n\nFirst, create some example data. This generates a cloud of points that\nloosely follows a quadratic curve:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(seq(-2, 2, length.out = 201), \"float32\")\n\nf <- function(x)\n  x^2 + 2*x - 5\n\nground_truth <- f(x) \ny <- ground_truth + tf$random$normal(shape(201))\n\nx %<>% as.array()\ny %<>% as.array()\nground_truth %<>% as.array()\n\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\"),\n       lty = c(NA, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\"))\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nCreate a model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel(tf$keras$Model) %py_class% {\n  initialize <- function(units) {\n    super$initialize()\n    self$dense1 <- layer_dense(\n      units = units,\n      activation = tf$nn$relu,\n      kernel_initializer = tf$random$normal,\n      bias_initializer = tf$random$normal\n    )\n    self$dense2 <- layer_dense(units = 1)\n  }\n  \n  call <- function(x, training = TRUE) {\n    x %>% \n      .[, tf$newaxis] %>% \n      self$dense1() %>% \n      self$dense2() %>% \n      .[, 1] \n  }\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- Model(64)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nuntrained_predictions <- model(as_tensor(x))\n\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlines(x, untrained_predictions, col = \"forestgreen\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\", \"forestgreen\"),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\", \"Untrained predictions\"))\ntitle(\"Before training\")\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nWrite a basic training loop:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvariables <- model$variables\n\noptimizer <- tf$optimizers$SGD(learning_rate=0.01)\n\nfor (step in seq(1000)) {\n  \n  with(tf$GradientTape() %as% tape, {\n    prediction <- model(x)\n    error <- (y - prediction) ^ 2\n    mean_error <- mean(error)\n  })\n  gradient <- tape$gradient(mean_error, variables)\n  optimizer$apply_gradients(zip_lists(gradient, variables))\n\n  if (step %% 100 == 0)\n    message(sprintf('Mean squared error: %.3f', as.array(mean_error)))\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.214\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.162\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.141\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.130\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.122\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.115\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.110\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.106\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.101\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nMean squared error: 1.098\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrained_predictions <- model(x)\nplot(x, y, type = 'p', col = \"deepskyblue2\", pch = 19)\nlines(x, ground_truth, col = \"tomato2\", lwd = 3)\nlines(x, trained_predictions, col = \"forestgreen\", lwd = 3)\nlegend(\"topleft\", \n       col = c(\"deepskyblue2\", \"tomato2\", \"forestgreen\"),\n       lty = c(NA, 1, 1), lwd = 3,\n       pch = c(19, NA), \n       legend = c(\"Data\", \"Ground Truth\", \"Trained predictions\"))\ntitle(\"After training\")\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nThat's working, but remember that implementations of common training\nutilities are available in the `tf$keras` module. So consider using\nthose before writing your own. To start with, the `compile` and `fit`\nmethods for Keras `Model`s implement a training loop for you:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_model <- Model(64)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_model %>% compile(\n  loss = tf$keras$losses$MSE,\n  optimizer = tf$optimizers$SGD(learning_rate = 0.01)\n)\n\nhistory <- new_model %>% \n  fit(x, y,\n      epochs = 100,\n      batch_size = 32,\n      verbose = 0)\n\nmodel$save('./my_model')\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history, metrics = 'loss', method = \"base\") \n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# see ?plot.keras_training_history for more options.\n```\n:::\n\n\nRefer to [Basic training loops](basic_training_loops.qmd) and the [Keras\nguide](https://www.tensorflow.org/guide/keras) for more details.\n\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.11.0 (~/.virtualenvs/r-tensorflow-website/lib/python3.10/site-packages/tensorflow)\nPython v3.10 (~/.virtualenvs/r-tensorflow-website/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.5 LTS\n\nMatrix products: default\nBLAS:   /home/tomasz/opt/R-4.2.1/lib/R/lib/libRblas.so\nLAPACK: /usr/lib/x86_64-linux-gnu/libmkl_intel_lp64.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] keras_2.9.0.9000      tensorflow_2.9.0.9000\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9             pillar_1.8.1           compiler_4.2.1        \n [4] base64enc_0.1-3        tools_4.2.1            zeallot_0.1.0         \n [7] digest_0.6.31          jsonlite_1.8.4         evaluate_0.18         \n[10] lifecycle_1.0.3        tibble_3.1.8           lattice_0.20-45       \n[13] pkgconfig_2.0.3        png_0.1-8              rlang_1.0.6           \n[16] Matrix_1.5-3           cli_3.4.1              yaml_2.3.6            \n[19] xfun_0.35              fastmap_1.1.0          stringr_1.5.0         \n[22] knitr_1.41             generics_0.1.3         vctrs_0.5.1           \n[25] htmlwidgets_1.5.4      rprojroot_2.0.3        grid_4.2.1            \n[28] reticulate_1.26-9000   glue_1.6.2             here_1.0.1            \n[31] R6_2.5.1               tfautograph_0.3.2.9000 fansi_1.0.3           \n[34] rmarkdown_2.18         magrittr_2.0.3         whisker_0.4.1         \n[37] backports_1.4.1        htmltools_0.5.4        tfruns_1.5.1          \n[40] utf8_1.2.2             stringi_1.7.8         \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\"-m pip freeze\"), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.3.0\nasttokens==2.2.1\nastunparse==1.6.3\nbackcall==0.2.0\ncachetools==5.2.0\ncertifi==2022.12.7\ncharset-normalizer==2.1.1\ndecorator==5.1.1\ndill==0.3.6\netils==0.9.0\nexecuting==1.2.0\nflatbuffers==22.12.6\ngast==0.4.0\ngoogle-auth==2.15.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.57.0\ngrpcio==1.51.1\nh5py==3.7.0\nidna==3.4\nimportlib-resources==5.10.1\nipython==8.7.0\njedi==0.18.2\nkaggle==1.5.12\nkeras==2.11.0\nkeras-tuner==1.1.3\nkt-legacy==1.0.4\nlibclang==14.0.6\nMarkdown==3.4.1\nMarkupSafe==2.1.1\nmatplotlib-inline==0.1.6\nnumpy==1.23.5\noauthlib==3.2.2\nopt-einsum==3.3.0\npackaging==22.0\npandas==1.5.2\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.3.0\npromise==2.3\nprompt-toolkit==3.0.36\nprotobuf==3.19.6\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\npydot==1.4.2\nPygments==2.13.0\npyparsing==3.0.9\npython-dateutil==2.8.2\npython-slugify==7.0.0\npytz==2022.6\nPyYAML==6.0\nrequests==2.28.1\nrequests-oauthlib==1.3.1\nrsa==4.9\nscipy==1.9.3\nsix==1.16.0\nstack-data==0.6.2\ntensorboard==2.11.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.11.0\ntensorflow-datasets==4.7.0\ntensorflow-estimator==2.11.0\ntensorflow-hub==0.12.0\ntensorflow-io-gcs-filesystem==0.28.0\ntensorflow-metadata==1.12.0\ntermcolor==2.1.1\ntext-unidecode==1.3\ntoml==0.10.2\ntqdm==4.64.1\ntraitlets==5.7.1\ntyping_extensions==4.4.0\nurllib3==1.26.13\nwcwidth==0.2.5\nWerkzeug==2.2.2\nwrapt==1.14.1\nzipp==3.11.0\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2022-12-16 \nPage render time: 34 seconds\n```\n:::\n:::\n:::\n\n",
    "supporting": [
      "basics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}