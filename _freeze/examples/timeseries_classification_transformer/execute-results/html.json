{
  "hash": "654d071e2fe34e4670c88b2289c7d352",
  "result": {
    "markdown": "---\ntitle: \"Timeseries classification with a Transformer model\"\nauthors:\n  - \"[Theodoros Ntakouris](https://github.com/ntakouris)\"\n  - \"[terrytangyuan](https://github.com/terrytangyuan) - R adaptation\"\n  - \"[t-kalinowski](https://github.com/t-kalinowski) - R adaptation\"\ndate-created: 2022/12/12\ndate-last-modified: 2023/2/12\ndescription: \"This notebook demonstrates how to do timeseries classification using a Transformer model.\"\ncategories: [timeseries]\naliases:\n  - ../guide/keras/examples/timeseries_classification_transformer/index.html\n---\n\n\n## Introduction\n\nThis is the Transformer architecture from [Attention Is All You\nNeed](https://arxiv.org/abs/1706.03762), applied to timeseries instead\nof natural language.\n\nThis example requires TensorFlow 2.4 or higher.\n\n## Load the dataset\n\nWe are going to use the same dataset and preprocessing as the\n[TimeSeries Classification from\nScratch](https://tensorflow.rstudio.com/examples/timeseries_classification_from_scratch)\nexample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nset.seed(1234)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA\"\n\ntrain_df <- \"FordA_TRAIN.tsv\" %>%\n  get_file(., file.path(url, .)) %>%\n  readr::read_tsv(col_names = FALSE)\nx_train <- as.matrix(train_df[, -1])\ny_train <- as.matrix(train_df[, 1])\n\ntest_df <- \"FordA_TEST.tsv\" %>%\n  get_file(., file.path(url, .)) %>%\n  readr::read_tsv(col_names = FALSE)\nx_test <- as.matrix(test_df[, -1])\ny_test <- as.matrix(test_df[, 1])\n\nn_classes <- length(unique(y_train))\n\nshuffle_ind <- sample(nrow(x_train))\nx_train <- x_train[shuffle_ind, , drop = FALSE]\ny_train <- y_train[shuffle_ind, , drop = FALSE]\n\ny_train[y_train == -1] <- 0\ny_test [y_test  == -1] <- 0\n\ndim(x_train) <- c(dim(x_train), 1)\ndim(x_test) <- c(dim(x_test), 1)\n```\n:::\n\n\n## Build a model\n\nOur model processes a tensor of shape\n`(batch size, sequence length, features)`, where `sequence length` is\nthe number of time steps and `features` is each input timeseries.\n\nYou can replace your classification RNN layers with this one: the inputs\nare fully compatible!\n\nWe include residual connections, layer normalization, and dropout. The\nresulting layer can be stacked multiple times. The projection layers are\nimplemented through `layer_conv_1d()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntransformer_encoder <- function(inputs,\n                                head_size,\n                                num_heads,\n                                ff_dim,\n                                dropout = 0) {\n  # Attention and Normalization\n  attention_layer <-\n    layer_multi_head_attention(key_dim = head_size,\n                               num_heads = num_heads,\n                               dropout = dropout)\n  \n  n_features <- dim(inputs) %>% tail(1)\n  \n  x <- inputs %>%\n    attention_layer(., .) %>%\n    layer_dropout(dropout) %>%\n    layer_layer_normalization(epsilon = 1e-6)\n  \n  res <- x + inputs\n  \n  # Feed Forward Part\n  x <- res %>%\n    layer_conv_1d(ff_dim, kernel_size = 1, activation = \"relu\") %>%\n    layer_dropout(dropout) %>%\n    layer_conv_1d(n_features, kernel_size = 1) %>%\n    layer_layer_normalization(epsilon = 1e-6)\n  \n  # return output + residual\n  x + res\n}\n\n\nbuild_model <- function(input_shape,\n                        head_size,\n                        num_heads,\n                        ff_dim,\n                        num_transformer_blocks,\n                        mlp_units,\n                        dropout = 0,\n                        mlp_dropout = 0) {\n  \n  inputs <- layer_input(input_shape)\n  \n  x <- inputs\n  for (i in 1:num_transformer_blocks) {\n    x <- x %>%\n      transformer_encoder(\n        head_size = head_size,\n        num_heads = num_heads,\n        ff_dim = ff_dim,\n        dropout = dropout\n      )\n  }\n  \n  x <- x %>% \n    layer_global_average_pooling_1d(data_format = \"channels_first\")\n  \n  for (dim in mlp_units) {\n    x <- x %>%\n      layer_dense(dim, activation = \"relu\") %>%\n      layer_dropout(mlp_dropout)\n  }\n  \n  outputs <- x %>% \n    layer_dense(n_classes, activation = \"softmax\")\n  \n  keras_model(inputs, outputs)\n}\n```\n:::\n\n\n## Train and evaluate\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninput_shape <- dim(x_train)[-1] # drop batch dim\nmodel <- build_model(\n  input_shape,\n  head_size = 256,\n  num_heads = 4,\n  ff_dim = 4,\n  num_transformer_blocks = 4,\n  mlp_units = c(128),\n  mlp_dropout = 0.4,\n  dropout = 0.25\n)\nmodel %>% compile(\n  loss = \"sparse_categorical_crossentropy\",\n  optimizer = optimizer_adam(learning_rate = 1e-4),\n  metrics = c(\"sparse_categorical_accuracy\")\n)\n\nmodel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"model\"\n____________________________________________________________________________\n Layer (type)            Output Shape    Param #  Connected to              \n============================================================================\n input_1 (InputLayer)    [(None, 500, 1  0        []                        \n                         )]                                                 \n multi_head_attention (M  (None, 500, 1)  7169    ['input_1[0][0]',         \n ultiHeadAttention)                                'input_1[0][0]']         \n dropout (Dropout)       (None, 500, 1)  0        ['multi_head_attention[0][\n                                                  0]']                      \n layer_normalization (La  (None, 500, 1)  2       ['dropout[0][0]']         \n yerNormalization)                                                          \n tf.math.add (TFOpLambda  (None, 500, 1)  0       ['layer_normalization[0][0\n )                                                ]',                       \n                                                   'input_1[0][0]']         \n conv1d_1 (Conv1D)       (None, 500, 4)  8        ['tf.math.add[0][0]']     \n dropout_1 (Dropout)     (None, 500, 4)  0        ['conv1d_1[0][0]']        \n conv1d (Conv1D)         (None, 500, 1)  5        ['dropout_1[0][0]']       \n layer_normalization_1 (  (None, 500, 1)  2       ['conv1d[0][0]']          \n LayerNormalization)                                                        \n tf.math.add_1 (TFOpLamb  (None, 500, 1)  0       ['layer_normalization_1[0]\n da)                                              [0]',                     \n                                                   'tf.math.add[0][0]']     \n multi_head_attention_1   (None, 500, 1)  7169    ['tf.math.add_1[0][0]',   \n (MultiHeadAttention)                              'tf.math.add_1[0][0]']   \n dropout_2 (Dropout)     (None, 500, 1)  0        ['multi_head_attention_1[0\n                                                  ][0]']                    \n layer_normalization_2 (  (None, 500, 1)  2       ['dropout_2[0][0]']       \n LayerNormalization)                                                        \n tf.math.add_2 (TFOpLamb  (None, 500, 1)  0       ['layer_normalization_2[0]\n da)                                              [0]',                     \n                                                   'tf.math.add_1[0][0]']   \n conv1d_3 (Conv1D)       (None, 500, 4)  8        ['tf.math.add_2[0][0]']   \n dropout_3 (Dropout)     (None, 500, 4)  0        ['conv1d_3[0][0]']        \n conv1d_2 (Conv1D)       (None, 500, 1)  5        ['dropout_3[0][0]']       \n layer_normalization_3 (  (None, 500, 1)  2       ['conv1d_2[0][0]']        \n LayerNormalization)                                                        \n tf.math.add_3 (TFOpLamb  (None, 500, 1)  0       ['layer_normalization_3[0]\n da)                                              [0]',                     \n                                                   'tf.math.add_2[0][0]']   \n multi_head_attention_2   (None, 500, 1)  7169    ['tf.math.add_3[0][0]',   \n (MultiHeadAttention)                              'tf.math.add_3[0][0]']   \n dropout_4 (Dropout)     (None, 500, 1)  0        ['multi_head_attention_2[0\n                                                  ][0]']                    \n layer_normalization_4 (  (None, 500, 1)  2       ['dropout_4[0][0]']       \n LayerNormalization)                                                        \n tf.math.add_4 (TFOpLamb  (None, 500, 1)  0       ['layer_normalization_4[0]\n da)                                              [0]',                     \n                                                   'tf.math.add_3[0][0]']   \n conv1d_5 (Conv1D)       (None, 500, 4)  8        ['tf.math.add_4[0][0]']   \n dropout_5 (Dropout)     (None, 500, 4)  0        ['conv1d_5[0][0]']        \n conv1d_4 (Conv1D)       (None, 500, 1)  5        ['dropout_5[0][0]']       \n layer_normalization_5 (  (None, 500, 1)  2       ['conv1d_4[0][0]']        \n LayerNormalization)                                                        \n tf.math.add_5 (TFOpLamb  (None, 500, 1)  0       ['layer_normalization_5[0]\n da)                                              [0]',                     \n                                                   'tf.math.add_4[0][0]']   \n multi_head_attention_3   (None, 500, 1)  7169    ['tf.math.add_5[0][0]',   \n (MultiHeadAttention)                              'tf.math.add_5[0][0]']   \n dropout_6 (Dropout)     (None, 500, 1)  0        ['multi_head_attention_3[0\n                                                  ][0]']                    \n layer_normalization_6 (  (None, 500, 1)  2       ['dropout_6[0][0]']       \n LayerNormalization)                                                        \n tf.math.add_6 (TFOpLamb  (None, 500, 1)  0       ['layer_normalization_6[0]\n da)                                              [0]',                     \n                                                   'tf.math.add_5[0][0]']   \n conv1d_7 (Conv1D)       (None, 500, 4)  8        ['tf.math.add_6[0][0]']   \n dropout_7 (Dropout)     (None, 500, 4)  0        ['conv1d_7[0][0]']        \n conv1d_6 (Conv1D)       (None, 500, 1)  5        ['dropout_7[0][0]']       \n layer_normalization_7 (  (None, 500, 1)  2       ['conv1d_6[0][0]']        \n LayerNormalization)                                                        \n tf.math.add_7 (TFOpLamb  (None, 500, 1)  0       ['layer_normalization_7[0]\n da)                                              [0]',                     \n                                                   'tf.math.add_6[0][0]']   \n global_average_pooling1  (None, 500)    0        ['tf.math.add_7[0][0]']   \n d (GlobalAveragePooling                                                    \n 1D)                                                                        \n dense (Dense)           (None, 128)     64128    ['global_average_pooling1d\n                                                  [0][0]']                  \n dropout_8 (Dropout)     (None, 128)     0        ['dense[0][0]']           \n dense_1 (Dense)         (None, 2)       258      ['dropout_8[0][0]']       \n============================================================================\nTotal params: 93,130\nTrainable params: 93,130\nNon-trainable params: 0\n____________________________________________________________________________\n```\n:::\n\n```{.r .cell-code}\ncallbacks <- list(\n  callback_early_stopping(patience = 10, restore_best_weights = TRUE))\n\nhistory <- model %>%\n  fit(\n    x_train,\n    y_train,\n    batch_size = 64,\n    epochs = 200,\n    callbacks = callbacks,\n    validation_split = 0.2\n  )\n\nmodel %>% evaluate(x_test, y_test, verbose = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                       loss sparse_categorical_accuracy \n                  0.3437351                   0.8522727 \n```\n:::\n:::\n\n\n## Conclusions\n\nIn about 110-120 epochs (25s each on Colab), the model reaches a\ntraining accuracy of \\~0.95, validation accuracy of \\~84 and a testing\naccuracy of \\~85, without hyperparameter tuning. And that is for a model\nwith less than 100k parameters. Of course, parameter count and accuracy\ncould be improved by a hyperparameter search and a more sophisticated\nlearning rate schedule, or a different optimizer.\n\nYou can use the trained model hosted on [Hugging Face\nHub](https://huggingface.co/keras-io/timeseries_transformer_classification)\nand try the demo on [Hugging Face\nSpaces](https://huggingface.co/spaces/keras-io/timeseries_transformer_classification).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}