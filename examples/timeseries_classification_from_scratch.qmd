---
title: "Timeseries classification from scratch"
authors:
  - "[hfawaz](https://github.com/hfawaz/)"
  - "[terrytangyuan](https://github.com/terrytangyuan) - R adaptation"
  - "[t-kalinowski](https://github.com/t-kalinowski) - R adaptation"
date-created: 2022/12/03
date-last-modified: 2022/12/03
description: "Training a timeseries classifier from scratch on the FordA dataset from the UCR/UEA archive."
categories: [timeseries]
aliases:
  - ../guide/keras/examples/timeseries_classification_from_scratch/index.html
---

## Introduction

This example shows how to do timeseries classification from scratch, starting from raw
CSV timeseries files on disk. We demonstrate the workflow on the FordA dataset from the
[UCR/UEA archive](https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/).

## Setup

```{r}
library(tensorflow)
library(keras)
set.seed(1234)
```

```{r, message=FALSE}
train_df <- readr::read_tsv("https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/FordA_TRAIN.tsv", col_names = FALSE)
x_train <- train_df[, -1]
y_train <- train_df[, 1]

test_df <- readr::read_tsv("https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/FordA_TEST.tsv", col_names = FALSE)
x_test <- test_df[, -1]
y_test <- test_df[, 1]
```

## Visualize the data

Here we visualize one timeseries example for each class in the dataset.

```{r}
library(ggfortify)

ts_obj <- ts(data.frame("Class -1" = as.numeric(x_train[1, ]), "Class 1" = as.numeric(x_train[2, ])))
autoplot(ts_obj, ts.geom = 'line', facets = FALSE)
```

## Standardize the data

Our timeseries are already in a single length (500). However, their values are
usually in various ranges. This is not ideal for a neural network;
in general we should seek to make the input values normalized.
For this specific dataset, the data is already z-normalized: each timeseries sample
has a mean equal to zero and a standard deviation equal to one. This type of
normalization is very common for timeseries classification problems, see
[Bagnall et al. (2016)](https://link.springer.com/article/10.1007/s10618-016-0483-9).

In order to use `sparse_categorical_crossentropy`, we will have to count
the number of classes beforehand.

```{r}
num_classes <- nrow(unique(y_train))
```

Now we shuffle the training set because we will be using the `validation_split` option
later when training.

```{r}
x_train <- x_train[sample(nrow(x_train)),]
y_train <- y_train[sample(nrow(y_train)),]
```

Standardize the labels to positive integers.
The expected labels will then be 0 and 1.

```{r}
y_train[y_train == -1, ] <- 0
y_test[y_test == -1, ] <- 0
```

Note that the timeseries data used here are univariate, meaning we only have one channel
per timeseries example.
We will therefore transform the timeseries into a multivariate one with one channel
using a simple reshaping via numpy.
This will allow us to construct a model that is easily applicable to multivariate time
series.

```{r}
x_train <- as.matrix(x_train)
dim(x_train) <- c(dim(x_train), 1)
x_test <- as.matrix(x_test)
dim(x_test) <- c(dim(x_test), 1)
```

## Build a model

We build a Fully Convolutional Neural Network originally proposed in
[this paper](https://arxiv.org/abs/1611.06455).
The implementation is based on the TF 2 version provided
[here](https://github.com/hfawaz/dl-4-tsc/).
The following hyperparameters (kernel_size, filters, the usage of BatchNorm) were found
via random search using [KerasTuner](https://github.com/keras-team/keras-tuner).

```{r}
input_shape <- dim(x_train)[-1]
input_layer <- layer_input(input_shape)

output_layer <- input_layer %>%
  # First convolutional layer
  layer_conv_1d(filters=64, kernel_size=3, padding="same")() %>% 
  layer_batch_normalization()() %>%
  layer_activation_relu()() %>%
  # Second convolutional layer
  layer_conv_1d(filters=64, kernel_size=3, padding="same")() %>% 
  layer_batch_normalization()() %>%
  layer_activation_relu()() %>%
  # Third convolutional layer
  layer_conv_1d(filters=64, kernel_size=3, padding="same")() %>% 
  layer_batch_normalization()() %>%
  layer_activation_relu()() %>%
  layer_global_average_pooling_1d()() %>%
  layer_dense(num_classes, activation = "softmax")

model <- keras_model(inputs=input_layer, outputs=output_layer)
```


## Train the model

```{r}
epochs <- 500
batch_size <- 32
callbacks <- list(
  callback_model_checkpoint("best_model.h5", save_best_only = TRUE, monitor = "val_loss"),
  callback_reduce_lr_on_plateau(monitor = "val_loss", factor = 0.5, patience = 20, min_lr = 0.0001),
  callback_early_stopping(monitor = "val_loss", patience = 50, verbose = 1)
)

model %>% compile(
  optimizer = "adam",
  loss = "sparse_categorical_crossentropy",
  metrics = list("sparse_categorical_accuracy")
)
model %>%
  fit(x_train, y_train,
    batch_size = batch_size,
    epochs = epochs,
    callbacks = callbacks,
    validation_split = 0.2,
    verbose = 1)
```

## Evaluate model on test data

```{r}
TODO
model = keras.models.load_model("best_model.h5")

test_loss, test_acc = model.evaluate(x_test, y_test)

print("Test accuracy", test_acc)
print("Test loss", test_loss)
```

## Plot the model's training and validation loss
