{
  "hash": "63634c078a742e0e4bef7944e6eaaad2",
  "result": {
    "markdown": "---\ntitle: Introduction to gradients and automatic differentiation\ndescription: > \n  Learn how to compute gradients with automatic\n  differentiation in TensorFlow, the capability that powers machine\n  learning algorithms such as backpropagation.\naliases:\n  - ../../tutorials/advanced/customization/autodiff/index.html\n---\n\n\n## Automatic Differentiation and Gradients\n\n[Automatic\ndifferentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\nis useful for implementing machine learning algorithms such as\n[backpropagation](https://en.wikipedia.org/wiki/Backpropagation) for\ntraining neural networks.\n\nIn this guide, you will explore ways to compute gradients with\nTensorFlow, especially in eager execution.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\n```\n:::\n\n\n## Computing gradients\n\nTo differentiate automatically, TensorFlow needs to remember what\noperations happen in what order during the *forward* pass. Then, during\nthe *backward pass*, TensorFlow traverses this list of operations in\nreverse order to compute gradients.\n\n## Gradient tapes\n\nTensorFlow provides the `tf$GradientTape()` API for automatic\ndifferentiation; that is, computing the gradient of a computation with\nrespect to some inputs, usually `tf$Variable`s. TensorFlow \"records\"\nrelevant operations executed inside the context of a `tf$GradientTape()`\nonto a \"tape\". TensorFlow then uses that tape to compute the gradients\nof a \"recorded\" computation using [reverse mode\ndifferentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n\nHere is a simple example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  y <- x ^ 2\n})\n```\n:::\n\n\nOnce you've recorded some operations, use\n`GradientTape$gradient(target, sources)` to calculate the gradient of\nsome target (often a loss) relative to some source (often the model's\nvariables):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# dy = 2x * dx\n\ndy_dx <- tape$gradient(y, x)\ndy_dx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(6.0, shape=(), dtype=float32)\n```\n:::\n:::\n\n\nThe above example uses scalars, but `tf$GradientTape` works as easily on\nany tensor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nw <- tf$Variable(tf$random$normal(c(3L, 2L)), name = 'w')\nb <- tf$Variable(tf$zeros(2L, dtype = tf$float32), name = 'b')\nx <- as_tensor(1:3, \"float32\", shape = c(1, 3))\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  y <- tf$matmul(x, w) + b\n  loss <- mean(y ^ 2)\n})\n```\n:::\n\n\nTo get the gradient of `loss` with respect to both variables, you can\npass both as sources to the `gradient` method. The tape is flexible\nabout how sources are passed and will accept any nested combination of\nlists or dictionaries and return the gradient structured the same way\n(see `tf$nest`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc(dl_dw, dl_db) %<-% tape$gradient(loss, c(w, b))\n```\n:::\n\n\nThe gradient with respect to each source has the shape of the source:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nw$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([3, 2])\n```\n:::\n\n```{.r .cell-code}\ndl_dw$shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorShape([3, 2])\n```\n:::\n:::\n\n\nHere is the gradient calculation again, this time passing a named list\nof variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_vars <- list(w = w,\n                b = b)\n\ngrad <- tape$gradient(loss, my_vars)\ngrad$b\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([2.6269841 7.24559  ], shape=(2), dtype=float32)\n```\n:::\n:::\n\n\n## Gradients with respect to a model\n\nIt's common to collect `tf$Variables` into a `tf$Module` or one of its\nsubclasses (`tf$keras$layers$Layer`, `tf$keras$Model`) for\n[checkpointing](checkpoint.qmd) and [exporting](saved_model.qmd).\n\nIn most cases, you will want to calculate gradients with respect to a\nmodel's trainable variables. Since all subclasses of `tf$Module`\naggregate their variables in the `Module$trainable_variables` property,\nyou can calculate these gradients in a few lines of code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlayer <- layer_dense(units = 2, activation = 'relu')\nx <- as_tensor(1:3, \"float32\", shape = c(1, -1))\n\nwith(tf$GradientTape() %as% tape, {\n  # Forward pass\n  y <- layer(x)\n  loss <- mean(y ^ 2)\n})\n\n# Calculate gradients with respect to every trainable variable\ngrad <- tape$gradient(loss, layer$trainable_variables)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (pair in zip_lists(layer$trainable_variables, grad)) {\n  c(var, g) %<-% pair\n  print(glue::glue('{var$name}, shape: {format(g$shape)}'))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndense/kernel:0, shape: (3, 2)\ndense/bias:0, shape: (2)\n```\n:::\n:::\n\n\n## Controlling what the tape watches\n\nThe default behavior is to record all operations after accessing a\ntrainable `tf$Variable`. The reasons for this are:\n\n-   The tape needs to know which operations to record in the forward\n    pass to calculate the gradients in the backwards pass.\n-   The tape holds references to intermediate outputs, so you don't want\n    to record unnecessary operations.\n-   The most common use case involves calculating the gradient of a loss\n    with respect to all a model's trainable variables.\n\nFor example, the following fails to calculate a gradient because the\n`tf$Tensor` is not \"watched\" by default, and the `tf$Variable` is not\ntrainable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A trainable variable\nx0 <- tf$Variable(3.0, name = 'x0')\n\n# Not trainable\nx1 <- tf$Variable(3.0, name = 'x1', trainable = FALSE)\n\n# Not a Variable: A variable + tensor returns a tensor.\nx2 <- tf$Variable(2.0, name = 'x2') + 1.0\n\n# Not a variable\nx3 <- as_tensor(3.0, name = 'x3')\n\nwith(tf$GradientTape() %as% tape, {\n  y <- (x0 ^ 2) + (x1 ^ 2) + (x2 ^ 2)\n})\n\ngrad <- tape$gradient(y, list(x0, x1, x2, x3))\n\nstr(grad)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 4\n $ :<tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n $ : NULL\n $ : NULL\n $ : NULL\n```\n:::\n:::\n\n\nYou can list the variables being watched by the tape using the\n`GradientTape$watched_variables` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntape$watched_variables()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n<tf.Variable 'x0:0' shape=() dtype=float32, numpy=3.0>\n```\n:::\n:::\n\n\n`tf$GradientTape` provides hooks that give the user control over what is\nor is not watched.\n\nTo record gradients with respect to a `tf$Tensor`, you need to call\n`GradientTape$watch(x)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(3.0)\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- x ^ 2\n})\n\n# dy = 2x * dx\ndy_dx <- tape$gradient(y, x)\nas.array(dy_dx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6\n```\n:::\n:::\n\n\nConversely, to disable the default behavior of watching all\n`tf$Variables`, set `watch_accessed_variables = FALSE` when creating the\ngradient tape. This calculation uses two variables, but only connects\nthe gradient for one of the variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx0 <- tf$Variable(0.0)\nx1 <- tf$Variable(10.0)\n\nwith(tf$GradientTape(watch_accessed_variables = FALSE) %as% tape, {\n  tape$watch(x1)\n  y0 <- sin(x0)\n  y1 <- tf$nn$softplus(x1)\n  y <- y0 + y1\n  ys <- sum(y)\n})\n```\n:::\n\n\nSince `GradientTape$watch` was not called on `x0`, no gradient is\ncomputed with respect to it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\ngrad <- tape$gradient(ys, list(x0 = x0, x1 = x1))\n\ncat('dy/dx0: ', grad$x0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndy/dx0: \n```\n:::\n\n```{.r .cell-code}\ncat('dy/dx1: ', as.array(grad$x1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndy/dx1:  0.9999546\n```\n:::\n:::\n\n\n## Intermediate results\n\nYou can also request gradients of the output with respect to\nintermediate values computed inside the `tf$GradientTape` context.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(3.0)\n\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\n# Use the tape to compute the gradient of z with respect to the\n# intermediate value y.\n# dz_dy = 2 * y and y = x ^ 2 = 9\ntape$gradient(z, y) |> as.array()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 18\n```\n:::\n:::\n\n\nBy default, the resources held by a `GradientTape` are released as soon\nas the `GradientTape$gradient` method is called. To compute multiple\ngradients over the same computation, create a gradient tape with\n`persistent = TRUE`. This allows multiple calls to the `gradient` method\nas resources are released when the tape object is garbage collected. For\nexample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(c(1, 3.0))\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n\n  tape$watch(x)\n  y <- x * x\n  z <- y * y\n})\n\nas.array(tape$gradient(z, x))  # c(4.0, 108.0); (4 * x^3 at x = c(1.0, 3.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   4 108\n```\n:::\n\n```{.r .cell-code}\nas.array(tape$gradient(y, x))  # c(2.0, 6.0);   (2 * x at x = c(1.0, 3.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 6\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(tape)   # Drop the reference to the tape\n```\n:::\n\n\n## Notes on performance\n\n-   There is a tiny overhead associated with doing operations inside a\n    gradient tape context. For most eager execution this will not be a\n    noticeable cost, but you should still use tape context around the\n    areas only where it is required.\n\n-   Gradient tapes use memory to store intermediate results, including\n    inputs and outputs, for use during the backwards pass.\n\n    For efficiency, some ops (like `ReLU`) don't need to keep their\n    intermediate results and they are pruned during the forward pass.\n    However, if you use `persistent = TRUE` on your tape, *nothing is\n    discarded* and your peak memory usage will be higher.\n\n## Gradients of non-scalar targets\n\nA gradient is fundamentally an operation on a scalar.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(2.0)\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  y0 <- x ^ 2\n  y1 <- 1 / x\n})\n\nas.array(tape$gradient(y0, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n\n```{.r .cell-code}\nas.array(tape$gradient(y1, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.25\n```\n:::\n:::\n\n\nThus, if you ask for the gradient of multiple targets, the result for\neach source is:\n\n-   The gradient of the sum of the targets, or equivalently\n-   The sum of the gradients of each target.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(2.0)\nwith(tf$GradientTape() %as% tape, {\n  y0 <- x^2\n  y1 <- 1 / x\n})\n\nas.array(tape$gradient(list(y0 = y0, y1 = y1), x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.75\n```\n:::\n:::\n\n\nSimilarly, if the target(s) are not scalar the gradient of the sum is\ncalculated:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(2)\n\nwith(tf$GradientTape() %as% tape, {\n  y <- x * c(3, 4)\n})\n\nas.array(tape$gradient(y, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7\n```\n:::\n:::\n\n\nThis makes it simple to take the gradient of the sum of a collection of\nlosses, or the gradient of the sum of an element-wise loss calculation.\n\nIf you need a separate gradient for each item, refer to\n[Jacobians](advanced_autodiff$ipynb#jacobians).\n\nIn some cases you can skip the Jacobian. For an element-wise\ncalculation, the gradient of the sum gives the derivative of each\nelement with respect to its input-element, since each element is\nindependent:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$linspace(-10.0, 10.0, as.integer(200+1))\n\nwith(tf$GradientTape() %as% tape, {\n  tape$watch(x)\n  y <- tf$nn$sigmoid(x)\n})\n\ndy_dx <- tape$gradient(y, x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(var in alist(x, y, dy_dx))\n  eval(bquote(.(var) <- as.array(.(var))))\nplot(NULL, xlim = range(x), ylim = range(y), ann=F, frame.plot = F)\nlines(x, y, col = \"royalblue\", lwd = 2)\nlines(x, dy_dx, col = \"coral\", lwd=2)\nlegend(\"topleft\", inset = .05,\n       expression(y, dy/dx),\n       col = c(\"royalblue\", \"coral\"), lwd = 2)\n```\n\n::: {.cell-output-display}\n![](autodiff_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n## Control flow\n\nBecause a gradient tape records operations as they are executed, Python\ncontrol flow is naturally handled (for example, `if` and `while`\nstatements).\n\nHere a different variable is used on each branch of an `if`. The\ngradient only connects to the variable that was used:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(1.0)\n\nv0 <- tf$Variable(2.0)\nv1 <- tf$Variable(2.0)\n\nwith(tf$GradientTape(persistent = TRUE) %as% tape, {\n  tape$watch(x)\n  if (as.logical(x > 0.0))\n    result <- v0\n  else\n    result <- v1 ^ 2\n})\n\nc(dv0, dv1) %<-% tape$gradient(result, list(v0, v1))\n\ndv0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor(1.0, shape=(), dtype=float32)\n```\n:::\n\n```{.r .cell-code}\ndv1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n\nJust remember that the control statements themselves are not\ndifferentiable, so they are invisible to gradient-based optimizers.\n\nDepending on the value of `x` in the above example, the tape either\nrecords `result = v0` or `result = v1 ^ 2`. The gradient with respect to\n`x` is always `NULL`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(dx <- tape$gradient(result, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n\n## Getting a gradient of `NULL`\n\nWhen a target is not connected to a source you will get a gradient of\n`NULL`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(2)\ny <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  z <- y * y\n})\ntape$gradient(z, x)\n```\n:::\n\n\nHere `z` is obviously not connected to `x`, but there are several\nless-obvious ways that a gradient can be disconnected.\n\n### 1. Replaced a variable with a tensor\n\nIn the section on [\"controlling what the tape watches\"](#watches) you\nsaw that the tape will automatically watch a `tf$Variable` but not a\n`tf$Tensor`.\n\nOne common error is to inadvertently replace a `tf$Variable` with a\n`tf$Tensor`, instead of using `Variable$assign` to update the\n`tf$Variable`. Here is an example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(2.0)\n\nfor (epoch in seq(2)) {\n\n  with(tf$GradientTape() %as% tape,\n       {  y <- x+1 })\n\n  cat(x$`__class__`$`__name__`, \": \")\n  print(tape$gradient(y, x))\n  x <- x + 1   # This should be `x$assign_add(1)`\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\nEagerTensor : NULL\n```\n:::\n:::\n\n\n### 2. Did calculations outside of TensorFlow\n\nThe tape can't record the gradient path if the calculation exits\nTensorFlow. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnp <- reticulate::import(\"numpy\", convert = FALSE)\nx <- tf$Variable(as_tensor(1:4, dtype=tf$float32, shape = c(2, 2)))\n\nwith(tf$GradientTape() %as% tape, {\n  x2 <- x ^ 2\n\n  # This step is calculated with NumPy\n  y <- np$mean(x2, axis = 0L)\n\n  # Like most tf ops, reduce_mean will cast the NumPy array to a constant tensor\n  # using `tf$convert_to_tensor`.\n  y <- tf$reduce_mean(y, axis = 0L)\n})\n\nprint(tape$gradient(y, x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n\n### 3. Took gradients through an integer or string\n\nIntegers and strings are not differentiable. If a calculation path uses\nthese data types there will be no gradient.\n\nNobody expects strings to be differentiable, but it's easy to\naccidentally create an `int` constant or variable if you don't specify\nthe `dtype`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- as_tensor(10L)\n\nwith(tf$GradientTape() %as% g, {\n  g$watch(x)\n  y <- x * x\n})\n\ng$gradient(y, x)\n```\n:::\n\n\n````default\nWARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\nWARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\nWARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n````\n\n\nTensorFlow doesn't automatically cast between types, so, in practice,\nyou'll often get a type error instead of a missing gradient.\n\n### 4. Took gradients through a stateful object\n\nState stops gradients. When you read from a stateful object, the tape\ncan only observe the current state, not the history that lead to it.\n\nA `tf$Tensor` is immutable. You can't change a tensor once it's created.\nIt has a *value*, but no *state*. All the operations discussed so far\nare also stateless: the output of a `tf$matmul` only depends on its\ninputs.\n\nA `tf$Variable` has internal state---its value. When you use the\nvariable, the state is read. It's normal to calculate a gradient with\nrespect to a variable, but the variable's state blocks gradient\ncalculations from going farther back. For example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx0 <- tf$Variable(3.0)\nx1 <- tf$Variable(0.0)\n\nwith(tf$GradientTape() %as% tape, {\n  # Update x1 <- x1 + x0.\n  x1$assign_add(x0)\n  # The tape starts recording from x1.\n  y <- x1^2   # y = (x1 + x0)^2\n})\n\n# This doesn't work.\nprint(tape$gradient(y, x0))  #dy/dx0 = 2*(x1 + x0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n\nSimilarly, `tf$data$Dataset` iterators and `tf$queue`s are stateful, and\nwill stop all gradients on tensors that pass through them.\n\n## No gradient registered\n\nSome `tf$Operation`s are **registered as being non-differentiable\\* and\nwill return `NULL`. Others have** no gradient registered\\*\\*.\n\nThe\n[`tf$raw_ops`](https://www.tensorflow.org/api_docs/python/tf/raw_ops)\npage shows which low-level ops have gradients registered.\n\nIf you attempt to take a gradient through a float op that has no\ngradient registered the tape will throw an error instead of silently\nreturning `NULL`. This way you know something has gone wrong.\n\nFor example, the `tf$image$adjust_contrast` function wraps\n`raw_ops$AdjustContrastv2`, which could have a gradient but the gradient\nis not implemented:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimage <- tf$Variable(array(c(0.5, 0, 0), c(1,1,1)))\ndelta <- tf$Variable(0.1)\n\nwith(tf$GradientTape() %as% tape, {\n  new_image <- tf$image$adjust_contrast(image, delta)\n})\n\ntry(print(tape$gradient(new_image, list(image, delta))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in py_call_impl(callable, dots$args, dots$keywords) : \n  LookupError: gradient registry has no entry for: AdjustContrastv2\n```\n:::\n:::\n\n\nIf you need to differentiate through this op, you'll either need to\nimplement the gradient and register it (using `tf$RegisterGradient`) or\nre-implement the function using other ops.\n\n## Zeros instead of NULL\n\nIn some cases it would be convenient to get 0 instead of `NULL` for\nunconnected gradients. You can decide what to return when you have\nunconnected gradients using the `unconnected_gradients` argument:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- tf$Variable(c(2, 2))\ny <- tf$Variable(3)\n\nwith(tf$GradientTape() %as% tape, {\n  z <- y^2\n})\ntape$gradient(z, x, unconnected_gradients = tf$UnconnectedGradients$ZERO)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntf.Tensor([0. 0.], shape=(2), dtype=float32)\n```\n:::\n:::\n---\nformat: html\n---\n\n## Environment Details\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Tensorflow Version\n\n::: {.cell}\n\n```{.r .cell-code}\ntensorflow::tf_config()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTensorFlow v2.11.0 (~/.virtualenvs/r-tensorflow-website/lib/python3.10/site-packages/tensorflow)\nPython v3.10 (~/.virtualenvs/r-tensorflow-website/bin/python)\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### R Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.2.1 (2022-06-23)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.5 LTS\n\nMatrix products: default\nBLAS:   /home/tomasz/opt/R-4.2.1/lib/R/lib/libRblas.so\nLAPACK: /usr/lib/x86_64-linux-gnu/libmkl_intel_lp64.so\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] keras_2.9.0.9000      tensorflow_2.9.0.9000\n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.9           pillar_1.8.1         compiler_4.2.1      \n [4] base64enc_0.1-3      tools_4.2.1          zeallot_0.1.0       \n [7] digest_0.6.31        jsonlite_1.8.4       evaluate_0.18       \n[10] lifecycle_1.0.3      tibble_3.1.8         lattice_0.20-45     \n[13] pkgconfig_2.0.3      png_0.1-8            rlang_1.0.6         \n[16] Matrix_1.5-3         cli_3.4.1            yaml_2.3.6          \n[19] xfun_0.35            fastmap_1.1.0        stringr_1.5.0       \n[22] knitr_1.41           generics_0.1.3       vctrs_0.5.1         \n[25] htmlwidgets_1.5.4    rprojroot_2.0.3      grid_4.2.1          \n[28] reticulate_1.26-9000 glue_1.6.2           here_1.0.1          \n[31] R6_2.5.1             fansi_1.0.3          rmarkdown_2.18      \n[34] magrittr_2.0.3       whisker_0.4.1        htmltools_0.5.4     \n[37] tfruns_1.5.1         utf8_1.2.2           stringi_1.7.8       \n[40] crayon_1.5.2        \n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Python Environment Information\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem2(reticulate::py_exe(), c(\"-m pip freeze\"), stdout = TRUE) |> writeLines()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nabsl-py==1.3.0\nasttokens==2.2.1\nastunparse==1.6.3\nbackcall==0.2.0\ncachetools==5.2.0\ncertifi==2022.12.7\ncharset-normalizer==2.1.1\ndecorator==5.1.1\ndill==0.3.6\netils==0.9.0\nexecuting==1.2.0\nflatbuffers==22.12.6\ngast==0.4.0\ngoogle-auth==2.15.0\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngoogleapis-common-protos==1.57.0\ngrpcio==1.51.1\nh5py==3.7.0\nidna==3.4\nimportlib-resources==5.10.1\nipython==8.7.0\njedi==0.18.2\nkaggle==1.5.12\nkeras==2.11.0\nkeras-tuner==1.1.3\nkt-legacy==1.0.4\nlibclang==14.0.6\nMarkdown==3.4.1\nMarkupSafe==2.1.1\nmatplotlib-inline==0.1.6\nnumpy==1.23.5\noauthlib==3.2.2\nopt-einsum==3.3.0\npackaging==22.0\npandas==1.5.2\nparso==0.8.3\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.3.0\npromise==2.3\nprompt-toolkit==3.0.36\nprotobuf==3.19.6\nptyprocess==0.7.0\npure-eval==0.2.2\npyasn1==0.4.8\npyasn1-modules==0.2.8\npydot==1.4.2\nPygments==2.13.0\npyparsing==3.0.9\npython-dateutil==2.8.2\npython-slugify==7.0.0\npytz==2022.6\nPyYAML==6.0\nrequests==2.28.1\nrequests-oauthlib==1.3.1\nrsa==4.9\nscipy==1.9.3\nsix==1.16.0\nstack-data==0.6.2\ntensorboard==2.11.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.11.0\ntensorflow-datasets==4.7.0\ntensorflow-estimator==2.11.0\ntensorflow-hub==0.12.0\ntensorflow-io-gcs-filesystem==0.28.0\ntensorflow-metadata==1.12.0\ntermcolor==2.1.1\ntext-unidecode==1.3\ntoml==0.10.2\ntqdm==4.64.1\ntraitlets==5.7.1\ntyping_extensions==4.4.0\nurllib3==1.26.13\nwcwidth==0.2.5\nWerkzeug==2.2.2\nwrapt==1.14.1\nzipp==3.11.0\n```\n:::\n:::\n:::\n\n::: {.callout-note appearance=\"simple\" collapse=\"true\"}\n### Additional Information\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nTF Devices:\n-  PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU') \n-  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') \nCPU cores: 12 \nDate rendered: 2022-12-16 \nPage render time: 5 seconds\n```\n:::\n:::\n:::\n\n",
    "supporting": [
      "autodiff_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}