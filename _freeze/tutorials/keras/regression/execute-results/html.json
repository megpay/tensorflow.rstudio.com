{
  "hash": "03312b5c7c508c46f11ebfc9a9aa3e1e",
  "result": {
    "markdown": "---\ntitle: Basic Regression\ndescription: Train a neural network to predict a continous value.\naliases:\n  - ../beginners/basic-ml/tutorial_basic_regression/index.html\n  - ../../articles/tutorial_basic_regression.html\n---\n\n\nIn a *regression* problem, the aim is to predict the output of a continuous value, like a price or a probability.\nContrast this with a *classification* problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).\n\nThis tutorial uses the classic [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles.\nTo do this, you will provide the models with a description of many automobiles from that time period.\nThis description includes attributes like cylinders, displacement, horsepower, and weight.\n\nThis example uses the Keras API.\n(Visit the Keras [tutorials](../keras) and [guides](../../guides/keras) to learn more.)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n:::\n\n\n## The Auto MPG dataset\n\nThe dataset is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/).\n\n### Get the data\n\nFirst download and import the dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\ncol_names <- c(\"mpg\",\"cylinders\",\"displacement\",\"horsepower\",\"weight\",\"acceleration\",\"model_year\", \"origin\",\"car_name\")\n\nraw_dataset <- read.table(\n  url,\n  header = T,\n  col.names = col_names,\n  na.strings = \"?\"\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- raw_dataset %>% select(-car_name)\ntail(dataset)\n```\n:::\n\n\n### Clean the data\n\nThe dataset contains a few unknown values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlapply(dataset, function(x) sum(is.na(x))) %>% str()\n```\n:::\n\n\nDrop those rows to keep this initial tutorial simple:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset <- na.omit(dataset)\n```\n:::\n\n\nThe `\"origin\"` column is categorical, not numeric.\nSo the next step is to one-hot encode the values in the column with the `recipes` package.\n\nNote: You can set up the `keras_model()` to do this kind of transformation for you but that's beyond the scope of this tutorial.\nCheck out the [Classify structured data using Keras preprocessing layers](../structured_data/preprocessing_layers.qmd) or [Load CSV data](../load_data/csv.qmd) tutorials for examples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(recipes)\ndataset <- recipe(mpg ~ ., dataset) %>%\n  step_num2factor(origin, levels = c(\"USA\", \"Europe\", \"Japan\")) %>%\n  step_dummy(origin, one_hot = TRUE) %>%\n  prep() %>%\n  bake(new_data = NULL)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(dataset)\n```\n:::\n\n\n### Split the data into training and test sets\n\nNow, split the dataset into a training set and a test set.\nYou will use the test set in the final evaluation of your models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplit <- initial_split(dataset, 0.8)\ntrain_dataset <- training(split)\ntest_dataset <- testing(split)\n```\n:::\n\n\n### Inspect the data\n\nReview the joint distribution of a few pairs of columns from the training set.\n\nThe top row suggests that the fuel efficiency (MPG) is a function of all the other parameters.\nThe other rows indicate they are functions of each other.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_dataset %>%\n  select(mpg, cylinders, displacement, weight) %>%\n  GGally::ggpairs()\n```\n:::\n\n\nLet's also check the overall statistics.\nNote how each feature covers a very different range:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskimr::skim(train_dataset)\n```\n:::\n\n\n### Split features from labels\n\nSeparate the target value---the \"label\"---from the features.\nThis label is the value that you will train the model to predict.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_features <- train_dataset %>% select(-mpg)\ntest_features <- test_dataset %>% select(-mpg)\n\ntrain_labels <- train_dataset %>% select(mpg)\ntest_labels <- test_dataset %>% select(mpg)\n```\n:::\n\n\n## Normalization\n\nIn the table of statistics it's easy to see how different the ranges of each feature are:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_skim <- skimr::skim_with(numeric = skimr::sfl(mean, sd))\ntrain_dataset %>%\n  select(where(~is.numeric(.x))) %>%\n  pivot_longer(\n    cols = everything(), names_to = \"variable\", values_to = \"values\") %>%\n  group_by(variable) %>%\n  summarise(mean = mean(values), sd = sd(values))\n```\n:::\n\n\nIt is good practice to normalize features that use different scales and ranges.\n\nOne reason this is important is because the features are multiplied by the model weights.\nSo, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n\nAlthough a model *might* converge without feature normalization, normalization makes training much more stable.\n\nNote: There is no advantage to normalizing the one-hot features---it is done here for simplicity.\nFor more details on how to use the preprocessing layers, refer to the [Working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers) guide and the [Classify structured data using Keras preprocessing layers](../structured_data/preprocessing_layers.qmd) tutorial.\n\n### The Normalization layer\n\nThe `layer_normalization()` is a clean and simple way to add feature normalization into your model.\n\nThe first step is to create the layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormalizer <- layer_normalization(axis = -1L)\n```\n:::\n\n\nThen, fit the state of the preprocessing layer to the data by calling `adapt()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormalizer %>% adapt(as.matrix(train_features))\n```\n:::\n\n\nCalculate the mean and variance, and store them in the layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(normalizer$mean)\n```\n:::\n\n\nWhen the layer is called, it returns the input data, with each feature independently normalized.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst <- as.matrix(train_features[1,])\n\ncat('First example:', first)\ncat('Normalized:', as.matrix(normalizer(first)))\n```\n:::\n\n\n## Linear regression\n\nBefore building a deep neural network model, start with linear regression using one and several variables.\n\n### Linear regression with one variable\n\nBegin with a single-variable linear regression to predict `'mpg'` from `'horsepower'`.\n\nTraining a model with Keras typically starts by defining the model architecture.\nUse a Sequential model, which [represents a sequence of steps](https://www.tensorflow.org/guide/keras/sequential_model).\n\nThere are two steps in your single-variable linear regression model:\n\n-   Normalize the `'horsepower'` input features using the `normalization` preprocessing layer.\n-   Apply a linear transformation ($y = mx+b$) to produce 1 output using a linear layer (`dense`).\n\nThe number of *inputs* can either be set by the `input_shape` argument, or automatically when the model is run for the first time.\n\nFirst, create a matrix made of the `'horsepower'` features.\nThen, instantiate the `layer_normalization` and fit its state to the `horsepower` data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhorsepower <- matrix(train_features$horsepower)\nhorsepower_normalizer <- layer_normalization(input_shape = shape(1), axis = NULL)\nhorsepower_normalizer %>% adapt(horsepower)\n```\n:::\n\n\nBuild the Keras Sequential model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhorsepower_model <- keras_model_sequential() %>%\n  horsepower_normalizer() %>%\n  layer_dense(units = 1)\n\nsummary(horsepower_model)\n```\n:::\n\n\nThis model will predict `'mpg'` from `'horsepower'`.\n\nRun the untrained model on the first 10 'horsepower' values.\nThe output won't be good, but notice that it has the expected shape of `(10, 1)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(horsepower_model, horsepower[1:10,])\n```\n:::\n\n\nOnce the model is built, configure the training procedure using the Keras `compile()` method.\nThe most important arguments to compile are the `loss` and the `optimizer`, since these define what will be optimized (`mean_absolute_error`) and how (using the `optimizer_adam`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhorsepower_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = 0.1),\n  loss = 'mean_absolute_error'\n)\n```\n:::\n\n\nUse Keras `fit()` to execute the training for 100 epochs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- horsepower_model %>% fit(\n  as.matrix(train_features$horsepower),\n  as.matrix(train_labels),\n  epochs = 100,\n  # Suppress logging.\n  verbose = 0,\n  # Calculate validation results on 20% of the training data.\n  validation_split = 0.2\n)\n```\n:::\n\n\nVisualize the model's training progress using the stats stored in the `history` object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n:::\n\n\nCollect the results on the test set for later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results <- list()\ntest_results[[\"horsepower_model\"]] <- horsepower_model %>% evaluate(\n  as.matrix(test_features$horsepower),\n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n\nSince this is a single variable regression, it's easy to view the model's predictions as a function of the input:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0, 250, length.out = 251)\ny <- predict(horsepower_model, x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_dataset) +\n  geom_point(aes(x = horsepower, y = mpg, color = \"data\")) +\n  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = \"prediction\"))\n```\n:::\n\n\n### Linear regression with multiple inputs\n\nYou can use an almost identical setup to make predictions based on multiple inputs.\nThis model still does the same $y = mx+b$ except that $m$ is a matrix and $b$ is a vector.\n\nCreate a two-step Keras Sequential model again with the first layer being `normalizer` (`layer_normalization(axis = -1)`) you defined earlier and adapted to the whole dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model <- keras_model_sequential() %>%\n  normalizer() %>%\n  layer_dense(units = 1)\n```\n:::\n\n\nWhen you call `predict()` on a batch of inputs, it produces `units = 1` outputs for each example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(linear_model, as.matrix(train_features[1:10, ]))\n```\n:::\n\n\nWhen you call the model, its weight matrices will be built---check that the `kernel` weights (the $m$ in $y = mx+b$) have a shape of `(9, 1)`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model$layers[[2]]$kernel\n```\n:::\n\n\nConfigure the model with Keras `compile()` and train with `fit()` for 100 epochs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_model %>% compile(\n  optimizer = optimizer_adam(learning_rate = 0.1),\n  loss = 'mean_absolute_error'\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- linear_model %>% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  epochs = 100,\n  # Suppress logging.\n  verbose = 0,\n  # Calculate validation results on 20% of the training data.\n  validation_split = 0.2\n)\n```\n:::\n\n\nUsing all the inputs in this regression model achieves a much lower training and validation error than the `horsepower_model`, which had one input:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n:::\n\n\nCollect the results on the test set for later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results[['linear_model']] <- linear_model %>%\n  evaluate(\n    as.matrix(test_features),\n    as.matrix(test_labels),\n    verbose = 0\n  )\n```\n:::\n\n\n## Regression with a deep neural network (DNN)\n\nIn the previous section, you implemented two linear models for single and multiple inputs.\n\nHere, you will implement single-input and multiple-input DNN models.\n\nThe code is basically the same except the model is expanded to include some \"hidden\" non-linear layers.\nThe name \"hidden\" here just means not directly connected to the inputs or outputs.\n\nThese models will contain a few more layers than the linear model:\n\n-   The normalization layer, as before (with `horsepower_normalizer` for a single-input model and `normalizer` for a multiple-input model).\n-   Two hidden, non-linear, `Dense` layers with the ReLU (`relu`) activation function nonlinearity.\n-   A linear `Dense` single-output layer.\n\nBoth models will use the same training procedure so the `compile` method is included in the `build_and_compile_model` function below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbuild_and_compile_model <- function(norm) {\n  model <- keras_model_sequential() %>%\n    norm() %>%\n    layer_dense(64, activation = 'relu') %>%\n    layer_dense(64, activation = 'relu') %>%\n    layer_dense(1)\n\n  model %>% compile(\n    loss = 'mean_absolute_error',\n    optimizer = optimizer_adam(0.001)\n  )\n\n  model\n}\n```\n:::\n\n\n### Regression using a DNN and a single input\n\nCreate a DNN model with only `'Horsepower'` as input and `horsepower_normalizer` (defined earlier) as the normalization layer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnn_horsepower_model <- build_and_compile_model(horsepower_normalizer)\n```\n:::\n\n\nThis model has quite a few more trainable parameters than the linear models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dnn_horsepower_model)\n```\n:::\n\n\nTrain the model with Keras `Model$fit`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- dnn_horsepower_model %>% fit(\n  as.matrix(train_features$horsepower),\n  as.matrix(train_labels),\n  validation_split = 0.2,\n  verbose = 0,\n  epochs = 100\n)\n```\n:::\n\n\nThis model does slightly better than the linear single-input `horsepower_model`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n:::\n\n\nIf you plot the predictions as a function of `'horsepower'`, you should notice how this model takes advantage of the nonlinearity provided by the hidden layers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0.0, 250, length.out = 251)\ny <- predict(dnn_horsepower_model, x)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_dataset) +\n  geom_point(aes(x = horsepower, y = mpg, color = \"data\")) +\n  geom_line(data = data.frame(x, y), aes(x = x, y = y, color = \"prediction\"))\n```\n:::\n\n\nCollect the results on the test set for later:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results[['dnn_horsepower_model']] <- dnn_horsepower_model %>% evaluate(\n  as.matrix(test_features$horsepower),\n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n\n### Regression using a DNN and multiple inputs\n\nRepeat the previous process using all the inputs.\nThe model's performance slightly improves on the validation dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnn_model <- build_and_compile_model(normalizer)\nsummary(dnn_model)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhistory <- dnn_model %>% fit(\n  as.matrix(train_features),\n  as.matrix(train_labels),\n  validation_split = 0.2,\n  verbose = 0,\n  epochs = 100\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(history)\n```\n:::\n\n\nCollect the results on the test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_results[['dnn_model']] <- dnn_model %>% evaluate(\n  as.matrix(test_features),\n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n\n## Performance\n\nSince all models have been trained, you can review their test set performance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(test_results, function(x) x)\n```\n:::\n\n\nThese results match the validation error observed during training.\n\n### Make predictions\n\nYou can now make predictions with the `dnn_model` on the test set using Keras `predict()` and review the loss:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predictions <- predict(dnn_model, as.matrix(test_features))\nggplot(data.frame(pred = as.numeric(test_predictions), mpg = test_labels$mpg)) +\n  geom_point(aes(x = pred, y = mpg)) +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\")\n```\n:::\n\n\nIt appears that the model predicts reasonably well.\n\nNow, check the error distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqplot(test_predictions - test_labels$mpg, geom = \"density\")\nerror <- test_predictions - test_labels\n```\n:::\n\n\nIf you're happy with the model, save it for later use with `Model$save`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_model_tf(dnn_model, 'dnn_model')\n```\n:::\n\n\nIf you reload the model, it gives identical output:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreloaded <- load_model_tf('dnn_model')\n\ntest_results[['reloaded']] <- reloaded %>% evaluate(\n  as.matrix(test_features),\n  as.matrix(test_labels),\n  verbose = 0\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsapply(test_results, function(x) x)\n```\n:::\n\n\n## Conclusion\n\nThis notebook introduced a few techniques to handle a regression problem.\nHere are a few more tips that may help:\n\n-   Mean squared error (MSE) (`loss_mean_squared_error()`) and mean absolute error (MAE) (`loss_mean_absolute_error()`) are common loss functions used for regression problems. MAE is less sensitive to outliers. Different loss functions are used for classification problems.\n-   Similarly, evaluation metrics used for regression differ from classification.\n-   When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n-   Overfitting is a common problem for DNN models, though it wasn't a problem for this tutorial. Visit the [Overfit and underfit](overfit_and_underfit.qmd) tutorial for more help with this.\n",
    "supporting": [
      "regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}